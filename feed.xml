<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://stan-haochen.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://stan-haochen.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-23T01:37:00+00:00</updated><id>https://stan-haochen.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">On Optimization in Deep Learning</title><link href="https://stan-haochen.github.io/blog/2016/optimization-in-rl/" rel="alternate" type="text/html" title="On Optimization in Deep Learning"/><published>2016-09-07T00:00:00+00:00</published><updated>2016-09-07T00:00:00+00:00</updated><id>https://stan-haochen.github.io/blog/2016/optimization-in-rl</id><content type="html" xml:base="https://stan-haochen.github.io/blog/2016/optimization-in-rl/"><![CDATA[<p>This is an old post which may not fit into modern view. Some recent finding such as lottery ticket theory is not covered in this post.</p> <p>There are at least exponentially many global minima for a neural net. Since permuating the nodes in one layer does not change the loss. Finding such points is not easy. Before certain techniques such as momentum came out, those nets were considered impossible to learn.</p> <p>Thanks to the constantly envolving hardwares and libraries, we do not have to worry about training time <em>that much</em> at least for convnets. Empirically, the non-convexity of neural nets seems not to be an issue. In practice, SGD works pretty well in optimizing very large networks even though the problem is proved to be NP-hard. However, researchers never stop studying the loss surface of deep neural nets and searching for better optimization strategies.</p> <p><a href="https://arxiv.org/abs/1605.07110">This paper</a> has been renewed on ArXiv recently, which leads me to <a href="https://news.ycombinator.com/item?id=11765111">this discussion</a>. Following are what I find interesting.</p> <h2 id="why-sgd-works">Why SGD works?</h2> <p>[Choromaska et al, AISTATS’15] (also [Dauphin et al, ICML’15] use tools from Statistical Physics to explain the behavior of stochastic gradient methods when training deep neural networks. This offers a macroscopic explanation of why SGD “works”, and gives a characterization of the network depth. The model is strongly simplified, and convolution is not considered.</p> <h3 id="saddle-points">Saddle points</h3> <p>We start from discussing saddle points, the vast majority of critical points on the error surfaces of neural networks.</p> <blockquote> <p>Here we argue, … that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum.</p> <p>– <cite> Dauphin et al, <a href="http://arxiv.org/abs/1406.2572">Identifying and attacking the saddle point problem in high-dimensional non-convex optimization</a> </cite></p> </blockquote> <p>The authors introduce saddle-free Newton method which requires the estimation of Hessian. They connect the loss function of a deep net to a high-dimensional Gaussian random field. They show that critical points with high training error are exponentially likely to be saddle points with many negative directions, and all local minima are likely to have error that is very close to that of the global minimum. (Described in <a href="https://arxiv.org/abs/1611.01838">Entropy-SGD: Biasing Gradient Descent Into Wide Valleys</a>.)</p> <p>The convergence of gradient descent is affected by the proliferation of saddle points surrounded by high error plateaus — as opposed to multiple local minima.</p> <blockquote> <p>The time spent by diffusion is inversely proportional to the smallest negative eigenvalue of the Hessian at a saddle point</p> <p>– <cite>Kramer’s law</cite></p> </blockquote> <blockquote> <p>It is believed that for many problems including learning deep nets, almost all local minimum have very similar function value to the global optimum, and hence finding a local minimum is good enough.</p> <p>– <cite> Rong Ge, <a href="http://www.offconvex.org/2016/03/22/saddlepoints/">Escaping from Saddle Points</a> </cite></p> </blockquote> <p>As the model grows deeper, local minima have loss closer to global minima. On the other hand, we do not care about global minimum because it often leads to overfitting.</p> <p>Saddle points exist along the paths between local minima, most objective functions have exponentially many of those. However, first order optimization algorithms may get stuck at saddle points. Strict saddle points can be escaped and global minima can be achieved in polynomial time (<a href="http://arxiv.org/abs/1503.02101">Ge et al., 2015</a>). Stochastic gradient introduces noise and help to push the current point away from saddle points.</p> <p>Non-convex problems can have ‘‘degenerate saddle points’’, whose Hessian is p.s.d. and have 0 eigenvalues. The performance of SGD on these kind of tasks is still not well studied.</p> <p>To conclude this part, AFAIK, we should care more about escaping from saddle point. And gradient based methods can do a better job than second-order methods in practice.</p> <h3 id="spin-glass-hamiltonian">Spin-glass Hamiltonian</h3> <p>See <a href="https://charlesmartin14.wordpress.com/2015/03/25/why-does-deep-learning-work/">Charles Martin: Why Does Deep Learning Works?</a> Both papers mentioned above use ideas from statistical physics and spin-glass models.</p> <table> <tbody> <tr> <td>Statistical physicists refer to $H_x(y)\equiv-\ln p(y</td> <td>x)$ as the <strong>Hamiltonian</strong>, quantifying the energy of $y$ given the parameter $x$. And $\mu\equiv -\ln p$ as <strong>self-information</strong>. We can rewrite Bayes’ formula as:</td> </tr> </tbody> </table> \[p(y) = \sigma(-H(y)-\mu)\] <p>We can see the features yield by a neural net as Hamiltonian and the softmax computes the classification probability.</p> <blockquote> <p>The long-term behavior of certain neural network models are governed by the statistical mechanism of infinite-range Ising spin-glass Hamiltonians</p> <p>– <cite> LeCun et. al., <a href="https://arxiv.org/abs/1412.0233">The Loss Surfaces of Multilayer Networks, 2015</a> </cite></p> </blockquote> <p>In this paper, he tries to explain the optimization paradigm with spin-glass theory.</p> <h3 id="implicit-bias-in-sgd">Implicit Bias in SGD</h3> <ul> <li><a href="https://arxiv.org/abs/1611.01838">Chaudhari</a> proposed a surrogate loss that explicitly biases SGD dynamics towards flat local minima. The corresponding algorithm relates closely to stochastic gradient Langevin dynamics.</li> <li>Another interpretation is that SGD performs Variational Inference (VI).</li> </ul> <h2 id="what-does-the-minima-look-like">What does the minima look like?</h2> <p>Take for example the concept of mode connectivity (<a href="https://arxiv.org/abs/1802.10026">Garipov et al, 2018</a>): it seems that the modes found by SGD using different random seeds are not just isolated basins, but they are connected by smooth valleys along which the training and test error are low.</p> <h3 id="no-poor-local-minima">No poor local minima</h3> <p><a href="https://arxiv.org/abs/1412.6544">Research at Google and Stanford</a> confirms that the Deep Learning Energy Landscapes appear to be roughly convex. A bolder hypothesis is that deep networks are spin funnels. And as the net gets larger, the funnel gets sharper. If this is true, our major concern should be to avoid over-training rather than the convexity of the network.</p> <p>Finally we arrive at the paper itself. Nets are optimized well by local gradient methods and seems not to be affected by local minima. The author claims that every local minimum is a global minimum and “bad” saddle points (degenerated ones) exists for deeper nets. Thm 2.3 gives clear result on linear networks.</p> <p>The main result Thm 3.2 generalizes <a href="https://arxiv.org/abs/1412.0233">Choromanska et al, 2015</a>’s idea for nonlinear network relies on 4 (seemingly strong) assumptions:</p> <ol> <li>The dimensionality of the output is smaller than the input.</li> <li>The inputs are random and decorrelated.</li> <li>A connection in the network is activated or not is random with the same probability of success across the network. (ReLU thresholding happens randomly.)</li> <li>The network activations are independent of the input, the weights and each other.</li> </ol> <p>They relax the majority of the asssumptions, which is very promising, but leave a weaker condition A1u-m and A5u-m (<a href="https://www.reddit.com/r/MachineLearning/comments/4ktqeu/160507110_deep_learning_without_poor_local_minima/">from reddit post</a>).</p> <p>Recently DeepMind came up with <a href="https://arxiv.org/abs/1611.06310">another paper</a> claiming the assumptions are too strong for real data. And devised counter examples with finite datatets for rectified MLPs. For finite sized models/datasets, one does not have a globally good behavior of learning regardless of the model size.</p> <p>Even though deep learning energy landscapes appear to be roughly convex, or as this post referred to, local minimal free, a deep model has to include more engineering details to aid its convergence. Problems such as covariance shift and overfitting still have to be handled by engineering techniques.</p> <h3 id="arriving-on-flatter-minima">Arriving on flatter minima</h3> <blockquote> <p>large-batch methods tend to converge to sharp minimizers of the training and testing functions – and that sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation.</p> <p>– <cite> <a href="https://stanstarks.github.io/tw5/#On%20Large-Batch%20Training%20for%20Deep%20Learning%3A%20Generalization%20Gap%20and%20Sharp%20Minima">On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</a> </cite></p> </blockquote> <ul> <li><a href="https://arxiv.org/abs/1802.06175">An Alternative View: When Does SGD Escape Local Minima?</a></li> </ul> <h2 id="should-2-nd-order-methods-ever-work">Should 2-nd order methods ever work?</h2> <p>Basiclly no. Because the Hessian vector product require very low variance estimation, which leads to batch size larger than 1000. But <a href="https://www.reddit.com/r/MachineLearning/comments/599wbr/project_i_accidentally_wrote_a_quasinewton_lbfgs/">some rare cases</a> happen when 2nd order methods with small batch size works.</p> <h2 id="gradient-starvation">Gradient Starvation</h2> <ul> <li><a href="https://arxiv.org/abs/1809.06848">On the Learning Dynamics of Deep Neural Networks</a> <ul> <li>Some features will dominate the gradient and sheding other equally important features.</li> </ul> </li> </ul>]]></content><author><name>Hao Chen</name></author><category term="optimization"/><summary type="html"><![CDATA[An old post about no local minima in Deep Learning.]]></summary></entry></feed>