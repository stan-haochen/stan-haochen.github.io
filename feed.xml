<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://stan-haochen.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://stan-haochen.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-02T13:50:33+00:00</updated><id>https://stan-haochen.github.io/feed.xml</id><title type="html">blank</title><subtitle>Hao Chen&apos;s personal academic website. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Faster and Finer Instance Segmentation With Blendmask</title><link href="https://stan-haochen.github.io/blog/2020/blendmask/" rel="alternate" type="text/html" title="Faster and Finer Instance Segmentation With Blendmask"/><published>2020-01-04T00:00:00+00:00</published><updated>2020-01-04T00:00:00+00:00</updated><id>https://stan-haochen.github.io/blog/2020/blendmask</id><content type="html" xml:base="https://stan-haochen.github.io/blog/2020/blendmask/"><![CDATA[<p>Update 01/05/2020:</p> <p>I have uploaded the CVPR Spotlight video to YouTube.</p> <figure> <iframe src="https://www.youtube.com/embed/fdFUFwrWzcQ" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"/> </figure> <hr/> <p>Update 20/03/2020:</p> <p>I give a talk on BlendMask <a href="https://live.bilibili.com/3344545">here</a> at 20:00 Beijing Time (UTC+8) 24/03/2020. You can download the slides <a href="https://cloudstor.aarnet.edu.au/plus/s/mSgeji3PQiD84OG">here</a>.</p> <hr/> <p>I want to briefly highlight our recent paper on instance segmentation:</p> <ul> <li>Hao Chen, Kunyang Sun, Zhi Tian, Chunhua Shen, Yongming Huang, Youliang Yan (2020) <a href="https://arxiv.org/abs/2001.00309">BlendMask: Top-Down Meets Bottom-Up for Instance Segmentation</a></li> </ul> <p>The motivation behind this paper is to proposal a general framework for instance-level tasks to reduce the per-instance computation in two-stage methods which could slows down the inference in complex senarios.</p> <h2 id="background">Background</h2> <p>Instance-level tasks such as instance segmentation, keypoint detection, tracking etc. all shares a similar procedure, detect-then-segment. That is, first use an object detection network to generate instance proposals and then for each instance, use a sub-network to predict the instance-level results. The advantange of this method against naive dense prediction is that for instances of different sizes, the features for the second stage is aligned (see <a href="https://arxiv.org/abs/1909.00169">this review by Oksuz et. al.</a>). Furthermore, in the second stage, only possible foreground features are computed in the second stage, which is more efficient and the sample imbalance problem is somehow mitigated (see <a href="https://arxiv.org/abs/1708.02002">Lin et. al.</a>).</p> <p>But the second-stage computation can be costly if we need highly detailed predictions (such as <a href="http://densepose.org/">DensePose</a> and high resolution instance segmentation like <a href="https://arxiv.org/abs/1912.08193">PointRend</a>).</p> <p>In BlendMask, we simplify the instance segmentation head of Mask R-CNN from a four-layer ConvNet to a tensor-product operation (called Blend) by reusing a densely predicted global segmentation mask. The framework resembles <a href="https://arxiv.org/abs/1904.02689">YOLACT</a> with a redesigned top module (called attention). We are able to achieve 10ms+ speedup while improving the mask AP for instance segmentation. One advantage of BlendMask is that <em>we can increase the instance output resolution almost for free</em>.</p> <h2 id="top-down-meets-bottom-up-middle-out">Top-down Meets Bottom-up (Middle-Out?)</h2> <p>Without loss of generality, we build BlendMask upon <a href="https://arxiv.org/abs/1904.01355">FCOS</a>, a widely adopted one-stage object detection framework, which by the way has a very supportive and active <a href="https://github.com/tianzhi0549/FCOS">github repo</a>. For instance segmentation, we add two modules, namely bottom and top to FCOS. These two modules are lightweight and flexible, allowing BlendMask to be incorporated into most object detection models.</p> <p>The nomenclature of BlendMask top and bottom modules is adopted from the top-down and bottom-up methodologies in instance detection. Top-down approaches rely on high-level features to predict the entire instance, for example predicting bounding box offsets with final prediction layers of one-stage object detectors (<a href="https://pjreddie.com/darknet/yolo/">YOLO</a>, FCOS etc.). Bottom-up approaches ensemble local predictions, grouping local pixels or keypoints into instances (<a href="https://arxiv.org/abs/1708.02551">embedding based instance segmentation</a>, <a href="https://arxiv.org/abs/1812.08008">OpenPose</a> etc.)</p> <p>The key trade-off here is the receptive field size. With large receptive field, top-down approaches excel in identifying instances but the fine-grained details are often lost. On the contrary, bottom-up approaches retains high-resolution local information but usually have trouble grouping. (Bottom-up instance segmentation methods typically fall behind two-stage ones, except the recent <a href="https://arxiv.org/abs/1912.04488">SOLO</a>.)</p> <p>It is naturally for us to consider merging these two approaches. YOLACT does exactly that. It utilizes a vector of mixture coefficients as the top module to linearly combine along the channels of the bottom module, a group of prototypes.</p> <p>Can we go one step further? To separate overlapping instances, it is important for the local features to encode relative positions. YOLACT training procedure does not handle this part explicitly. And the top module is too simple that cannot provide enough instance level information.</p> <p>We make the top module more expressive by encoding the instance pose information. The idea is remotely relative to <a href="https://arxiv.org/abs/1603.08678">InstanceFCN</a> and <a href="https://arxiv.org/abs/1611.07709">FCIS</a>, which encode relative position information by spliting each instance into $K\times K$ tiles. The final segmentation is cropped from $K\times K$ feature maps and combined.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/blendmask/instancefcn-480.webp 480w,/assets/img/posts/blendmask/instancefcn-800.webp 800w,/assets/img/posts/blendmask/instancefcn-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/blendmask/instancefcn.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We make this process parametric by using self-attention instead of hard one-hot weights, and contiuous, using bilinear upsampling for the attention.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/blendmask/blender-480.webp 480w,/assets/img/posts/blendmask/blender-800.webp 800w,/assets/img/posts/blendmask/blender-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/blendmask/blender.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The blender module effectively reduces the channel size of YOLACT protonet, from 32 to 4, and produces better masks.</p> <p>Here is a live view of the blending process:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/blendmask/teaser-480.webp 480w,/assets/img/posts/blendmask/teaser-800.webp 800w,/assets/img/posts/blendmask/teaser-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/blendmask/teaser.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="qualitative-and-quantitative-results">Qualitative and Quantitative Results</h2> <p>Our model produces higher quality masks than Mask R-CNN, especially in the following cases:</p> <ul> <li>Large objects with complex shapes (Horse ears, human poses). Mask R-CNN fails to provide sharp borders.</li> <li>Objects in separated parts (tennis players occluded by nets, trains divided by poles). Mask R-CNN tends to include occlusions as false positive or segment targets into separate objects.</li> <li>Overlapping objects (riders, crowds, drivers). Mask R-CNN gets uncertain on the borders and leaves larger false negative regions. Sometimes, it assigns parts to the wrong objects, such as the last example in the first row.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/blendmask/qualitative-480.webp 480w,/assets/img/posts/blendmask/qualitative-800.webp 800w,/assets/img/posts/blendmask/qualitative-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/blendmask/qualitative.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Our model surpasses Mask R-CNN in AP while being more efficient. Furthermore, it is very natural to generalize our model to other instance-level tasks such as panoptic segmentation and tracking.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/blendmask/quantitative-480.webp 480w,/assets/img/posts/blendmask/quantitative-800.webp 800w,/assets/img/posts/blendmask/quantitative-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/blendmask/quantitative.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Similar to Mask R-CNN, we use RoIPooler to locate instances and extract features. We reduce the running time by moving the computation of R-CNN heads before the RoI sampling to generate position-sensitive feature maps. Repeated mask representation and computation for overlapping proposals are avoided.</p> <p>Another advantage of BlendMask is that it can produce higher quality masks, since our output resolution is not restricted by the top-level sampling. Increasing the RoIPooler resolution of Mask R-CNN will introduce the following problem. The head computation increases quadratically with respect to the RoI size. Larger RoIs requires deeper head structures. Different from dense pixel predictions, RoI foreground predictor has to be aware of whole instance-level information to distinguish foreground from other over-lapping instances. Thus, the larger the feature sizes are, the deeper sub-networks is needed.</p> <p>Here is a demo video with BlendMask.</p> <figure> <iframe src="https://www.youtube.com/embed/E-gXL-eIPCw" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"/> </figure> <p>For more results, please see <a href="https://arxiv.org/abs/2001.00309">our paper</a>.</p>]]></content><author><name>Hao Chen</name></author><category term="perception"/><summary type="html"><![CDATA[Introduction to our new instance segmentation model, BlendMask, which generates higher qualities than Mask R-CNN while being faster in inference.]]></summary></entry><entry><title type="html">NAS - Where Are We Now</title><link href="https://stan-haochen.github.io/blog/2019/nas/" rel="alternate" type="text/html" title="NAS - Where Are We Now"/><published>2019-12-04T00:00:00+00:00</published><updated>2019-12-04T00:00:00+00:00</updated><id>https://stan-haochen.github.io/blog/2019/nas</id><content type="html" xml:base="https://stan-haochen.github.io/blog/2019/nas/"><![CDATA[<blockquote> <p>First off this ain’t no diss record<br/> This for some of my homies that were misrepresented</p> <p>– <cite> Nas, Where Are They Now. Hip Hop is Dead, 2006. </cite></p> </blockquote> <p>For the past year and a half, I have been working on Neural Architecture Search (NAS). The idea of automatically designing neural networks for specific tasks is enticing for both practitioners and theorists. In production, NAS extends the scope of network pruning/compression and can benefits on chip energy saving modeling, etc. In research, NAS has raised new questions and challenges for convergence and generalization analysis, since it requires rapid and accurate structure evaluation.</p> <p>To quickly recap what’s going on with NAS, I suggest reading <a href="https://drsleep.github.io/NAS-at-CVPR-2019/">Vladimir’s post</a>. A curated list of literature on NAS is maintained <a href="https://www.automl.org/automl/literature-on-neural-architecture-search/">here</a>.</p> <p>In this post, I will cast NAS as a bi-level optimization problem. We want to minimize some function $f$, to achieve optimal accuracy or some complex objective considering speed-accuracy tradeoff, with respect to some hyperparameter $h$, in our case, the network structure. To simplify the analysis, we assume $h$ takes form of a sequence with length $L$ and vocabulary size $K$.</p> <p>\(\min_{h, z} f(z;h)\qquad s.t. \quad z = \operatorname{argmax}_{\theta_h} f(\theta_h;h).\) Two major problems NAS deals with are</p> <ol> <li>Inner loop is slow. We have to train a network with structure $h$.</li> <li>Since there is no explicit derivative, we cannot optimize $f(h)$ directly.</li> </ol> <h2 id="nas-with-variational-optimization">NAS with Variational Optimization</h2> <p>Straightforwardly, we can solve these two problems one by one. First, we minimize the upper bound of our objective:</p> \[\min_h f(h)\le \min_\alpha \mathbb E_{h\sim p_{\alpha}(h)}[f(h)],\] <p>where $p(h|\alpha)$ can be parametrized by a sequential network, of which the gradient becomes tractable: \(\nabla_\alpha \mathbb E_{p_\alpha(h)}[f(h)] = \mathbb E_{p_\alpha (h)}[f(h)\nabla_\alpha \log {p_\alpha}(h)].\)</p> <p>This is the REINFORCE algorithm used by <a href="https://arxiv.org/abs/1611.01578">Zoph and Le</a>. The gradient estimation can be made more efficient with PPO as in <a href="https://arxiv.org/abs/1707.07012">their later work</a>.</p> <p>In NAS, sample efficiency is a bigger issue than in normal reinforcement learning tasks. Because training a network can be as costly as it can get to evaluate a single action. In other words, we prefer lower variance searching algorithms than lower bias ones. This is the reason I don’t consider using evolutionary strategy or random search (such as hyperband) for NAS, which ususally requires more samples. According to my experience, to find a good architecture with length $L=20$ and $K=7$ takes about 3,000 samples with REINFORCE and 1,500 with PPO.</p> <p>Speeding up sample evaluation is definitely important. Typically, a proxy task is designed, which includes training a smaller model with smaller input resolution and less iterations. Some other tricks are analyzed by <a href="https://arxiv.org/abs/1810.10804">Nekrasove et al.</a> However, all these tricks introduce biases to the evaluation. <em>It is a good practice to analyse the generalization quality of the proxy tasks to the target task.</em></p> <h2 id="nas-with-discrete-structure-learning">NAS with Discrete Structure Learning</h2> <p>Another solution to the two problems is to consider them as one and solve them in one shot. The idea is to consider the structure parameters $h$ as a part of the network and one-shot the search by performing a network optimization, usually with SGD.</p> <p><a href="https://arxiv.org/abs/1806.09055">DARTS</a> uses a continous relaxation $h\approx \sigma(\alpha)$ on the operations, \(\nabla_\alpha \mathbb E_{p_\alpha(h)}[f(h)]\approx\nabla_\alpha f(\sigma(\alpha))\) where $\sigma$ is softmax activation. Although biased, This is reasonable considering the popular <a href="https://arxiv.org/abs/1803.03635">Lottery Ticket Hypothesis</a>. (I will comeback to this part later.) However, I consider the connection learning part to be ad hoc, simply selecting the highest two activations, to follow the cell-based search space in [<a href="(https://arxiv.org/abs/1611.01578)">Zoph and Le</a>].</p> <p>There are still a lot of unanswered questions. Is this approximation error bounded? How can we avoid overfitting? We don’t even bother developing more accurate gradient computation including inverse Hessian for the second-order optimization, probably because of the accurate gradient does not leads to better result because of this bias.</p> <p>This challenging questions require better understanding of the optimization mechanisms and properties, e.g. how to early stop? how does training affect generalization?</p> <p>Another possible fix to this biased estimation is discrete latent structure learning. [<a href="https://openreview.net/forum?id=rylqooRqK7">Xie et al.</a>] uses Gumbel-softmax trick to reduce this bias. \(\nabla_\alpha \mathbb E_{p_\alpha(h)}[f(h)]\approx \mathbb E_{p(u)}\nabla_\alpha f(\sigma(z/t));\quad z:=\log\frac{\alpha}{1-\alpha} + \log\frac{u}{1-u};\quad u\sim\operatorname{Uniform}(0, 1).\) A problem with this trick is that the variance goes to infinity as bias gets closer to $0$, which is controlled by the temperature $t$. I am interested to see someone combine this trick with control variates, such as in <a href="https://github.com/duvenaud/relax">relax</a>.</p>]]></content><author><name>Hao Chen</name></author><category term="probabilistic"/><summary type="html"><![CDATA[The reasons why people are either running towards or away from Neural Architecture Search.]]></summary></entry><entry><title type="html">On Optimization in Deep Learning</title><link href="https://stan-haochen.github.io/blog/2016/optimization-in-rl/" rel="alternate" type="text/html" title="On Optimization in Deep Learning"/><published>2016-09-07T00:00:00+00:00</published><updated>2016-09-07T00:00:00+00:00</updated><id>https://stan-haochen.github.io/blog/2016/optimization-in-rl</id><content type="html" xml:base="https://stan-haochen.github.io/blog/2016/optimization-in-rl/"><![CDATA[<p>This is an old post which may not fit into modern view. Some recent finding such as lottery ticket theory is not covered in this post.</p> <p>There are at least exponentially many global minima for a neural net. Since permuating the nodes in one layer does not change the loss. Finding such points is not easy. Before certain techniques such as momentum came out, those nets were considered impossible to learn.</p> <p>Thanks to the constantly envolving hardwares and libraries, we do not have to worry about training time <em>that much</em> at least for convnets. Empirically, the non-convexity of neural nets seems not to be an issue. In practice, SGD works pretty well in optimizing very large networks even though the problem is proved to be NP-hard. However, researchers never stop studying the loss surface of deep neural nets and searching for better optimization strategies.</p> <p><a href="https://arxiv.org/abs/1605.07110">This paper</a> has been renewed on ArXiv recently, which leads me to <a href="https://news.ycombinator.com/item?id=11765111">this discussion</a>. Following are what I find interesting.</p> <h2 id="why-sgd-works">Why SGD works?</h2> <p>[Choromaska et al, AISTATS’15] (also [Dauphin et al, ICML’15] use tools from Statistical Physics to explain the behavior of stochastic gradient methods when training deep neural networks. This offers a macroscopic explanation of why SGD “works”, and gives a characterization of the network depth. The model is strongly simplified, and convolution is not considered.</p> <h3 id="saddle-points">Saddle points</h3> <p>We start from discussing saddle points, the vast majority of critical points on the error surfaces of neural networks.</p> <blockquote> <p>Here we argue, … that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum.</p> <p>– <cite> Dauphin et al, <a href="http://arxiv.org/abs/1406.2572">Identifying and attacking the saddle point problem in high-dimensional non-convex optimization</a> </cite></p> </blockquote> <p>The authors introduce saddle-free Newton method which requires the estimation of Hessian. They connect the loss function of a deep net to a high-dimensional Gaussian random field. They show that critical points with high training error are exponentially likely to be saddle points with many negative directions, and all local minima are likely to have error that is very close to that of the global minimum. (Described in <a href="https://arxiv.org/abs/1611.01838">Entropy-SGD: Biasing Gradient Descent Into Wide Valleys</a>.)</p> <p>The convergence of gradient descent is affected by the proliferation of saddle points surrounded by high error plateaus — as opposed to multiple local minima.</p> <blockquote> <p>The time spent by diffusion is inversely proportional to the smallest negative eigenvalue of the Hessian at a saddle point</p> <p>– <cite>Kramer’s law</cite></p> </blockquote> <blockquote> <p>It is believed that for many problems including learning deep nets, almost all local minimum have very similar function value to the global optimum, and hence finding a local minimum is good enough.</p> <p>– <cite> Rong Ge, <a href="http://www.offconvex.org/2016/03/22/saddlepoints/">Escaping from Saddle Points</a> </cite></p> </blockquote> <p>As the model grows deeper, local minima have loss closer to global minima. On the other hand, we do not care about global minimum because it often leads to overfitting.</p> <p>Saddle points exist along the paths between local minima, most objective functions have exponentially many of those. However, first order optimization algorithms may get stuck at saddle points. Strict saddle points can be escaped and global minima can be achieved in polynomial time (<a href="http://arxiv.org/abs/1503.02101">Ge et al., 2015</a>). Stochastic gradient introduces noise and help to push the current point away from saddle points.</p> <p>Non-convex problems can have ‘‘degenerate saddle points’’, whose Hessian is p.s.d. and have 0 eigenvalues. The performance of SGD on these kind of tasks is still not well studied.</p> <p>To conclude this part, AFAIK, we should care more about escaping from saddle point. And gradient based methods can do a better job than second-order methods in practice.</p> <h3 id="spin-glass-hamiltonian">Spin-glass Hamiltonian</h3> <p>See <a href="https://charlesmartin14.wordpress.com/2015/03/25/why-does-deep-learning-work/">Charles Martin: Why Does Deep Learning Works?</a> Both papers mentioned above use ideas from statistical physics and spin-glass models.</p> <table> <tbody> <tr> <td>Statistical physicists refer to $H_x(y)\equiv-\ln p(y</td> <td>x)$ as the <strong>Hamiltonian</strong>, quantifying the energy of $y$ given the parameter $x$. And $\mu\equiv -\ln p$ as <strong>self-information</strong>. We can rewrite Bayes’ formula as:</td> </tr> </tbody> </table> \[p(y) = \sigma(-H(y)-\mu)\] <p>We can see the features yield by a neural net as Hamiltonian and the softmax computes the classification probability.</p> <blockquote> <p>The long-term behavior of certain neural network models are governed by the statistical mechanism of infinite-range Ising spin-glass Hamiltonians</p> <p>– <cite> LeCun et. al., <a href="https://arxiv.org/abs/1412.0233">The Loss Surfaces of Multilayer Networks, 2015</a> </cite></p> </blockquote> <p>In this paper, he tries to explain the optimization paradigm with spin-glass theory.</p> <h3 id="implicit-bias-in-sgd">Implicit Bias in SGD</h3> <ul> <li><a href="https://arxiv.org/abs/1611.01838">Chaudhari</a> proposed a surrogate loss that explicitly biases SGD dynamics towards flat local minima. The corresponding algorithm relates closely to stochastic gradient Langevin dynamics.</li> <li>Another interpretation is that SGD performs Variational Inference (VI).</li> </ul> <h2 id="what-does-the-minima-look-like">What does the minima look like?</h2> <p>Take for example the concept of mode connectivity (<a href="https://arxiv.org/abs/1802.10026">Garipov et al, 2018</a>): it seems that the modes found by SGD using different random seeds are not just isolated basins, but they are connected by smooth valleys along which the training and test error are low.</p> <h3 id="no-poor-local-minima">No poor local minima</h3> <p><a href="https://arxiv.org/abs/1412.6544">Research at Google and Stanford</a> confirms that the Deep Learning Energy Landscapes appear to be roughly convex. A bolder hypothesis is that deep networks are spin funnels. And as the net gets larger, the funnel gets sharper. If this is true, our major concern should be to avoid over-training rather than the convexity of the network.</p> <p>Finally we arrive at the paper itself. Nets are optimized well by local gradient methods and seems not to be affected by local minima. The author claims that every local minimum is a global minimum and “bad” saddle points (degenerated ones) exists for deeper nets. Thm 2.3 gives clear result on linear networks.</p> <p>The main result Thm 3.2 generalizes <a href="https://arxiv.org/abs/1412.0233">Choromanska et al, 2015</a>’s idea for nonlinear network relies on 4 (seemingly strong) assumptions:</p> <ol> <li>The dimensionality of the output is smaller than the input.</li> <li>The inputs are random and decorrelated.</li> <li>A connection in the network is activated or not is random with the same probability of success across the network. (ReLU thresholding happens randomly.)</li> <li>The network activations are independent of the input, the weights and each other.</li> </ol> <p>They relax the majority of the asssumptions, which is very promising, but leave a weaker condition A1u-m and A5u-m (<a href="https://www.reddit.com/r/MachineLearning/comments/4ktqeu/160507110_deep_learning_without_poor_local_minima/">from reddit post</a>).</p> <p>Recently DeepMind came up with <a href="https://arxiv.org/abs/1611.06310">another paper</a> claiming the assumptions are too strong for real data. And devised counter examples with finite datatets for rectified MLPs. For finite sized models/datasets, one does not have a globally good behavior of learning regardless of the model size.</p> <p>Even though deep learning energy landscapes appear to be roughly convex, or as this post referred to, local minimal free, a deep model has to include more engineering details to aid its convergence. Problems such as covariance shift and overfitting still have to be handled by engineering techniques.</p> <h3 id="arriving-on-flatter-minima">Arriving on flatter minima</h3> <blockquote> <p>large-batch methods tend to converge to sharp minimizers of the training and testing functions – and that sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation.</p> <p>– <cite> <a href="https://stanstarks.github.io/tw5/#On%20Large-Batch%20Training%20for%20Deep%20Learning%3A%20Generalization%20Gap%20and%20Sharp%20Minima">On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</a> </cite></p> </blockquote> <ul> <li><a href="https://arxiv.org/abs/1802.06175">An Alternative View: When Does SGD Escape Local Minima?</a></li> </ul> <h2 id="should-2-nd-order-methods-ever-work">Should 2-nd order methods ever work?</h2> <p>Basiclly no. Because the Hessian vector product require very low variance estimation, which leads to batch size larger than 1000. But <a href="https://www.reddit.com/r/MachineLearning/comments/599wbr/project_i_accidentally_wrote_a_quasinewton_lbfgs/">some rare cases</a> happen when 2nd order methods with small batch size works.</p> <h2 id="gradient-starvation">Gradient Starvation</h2> <ul> <li><a href="https://arxiv.org/abs/1809.06848">On the Learning Dynamics of Deep Neural Networks</a> <ul> <li>Some features will dominate the gradient and sheding other equally important features.</li> </ul> </li> </ul>]]></content><author><name>Hao Chen</name></author><category term="optimization"/><summary type="html"><![CDATA[An old post about no local minima in Deep Learning.]]></summary></entry></feed>