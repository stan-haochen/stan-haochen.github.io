<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://stan-haochen.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://stan-haochen.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-23T01:10:28+00:00</updated><id>https://stan-haochen.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://stan-haochen.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://stan-haochen.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://stan-haochen.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[<p>May 14, 2024 We‚Äôre introducing a series of updates across the Gemini family of models, including the new 1.5 Flash, our lightweight model for speed and efficiency, and Project Astra, our vision for the future of AI assistants. In December, we launched our first natively multimodal model Gemini 1.0 in three sizes: Ultra, Pro and Nano. Just a few months later we released 1.5 Pro, with enhanced performance and a breakthrough long context window of 1 million tokens.Developers and enterprise customers have been putting 1.5 Pro to use in incredible ways and finding its long context window, multimodal reasoning capabilities and impressive overall performance incredibly useful.We know from user feedback that some applications need lower latency and a lower cost to serve. This inspired us to keep innovating, so today, we‚Äôre introducing Gemini 1.5 Flash: a model that‚Äôs lighter-weight than 1.5 Pro, and designed to be fast and efficient to serve at scale.Both 1.5 Pro and 1.5 Flash are available in public preview with a 1 million token context window in Google AI Studio and Vertex AI. And now, 1.5 Pro is also available with a 2 million token context window via waitlist to developers using the API and to Google Cloud customers.We‚Äôre also introducing updates across the Gemini family of models, announcing our next generation of open models, Gemma 2, and sharing progress on the future of AI assistants, with Project Astra.Context lengths of leading foundation models compared with Gemini 1.5‚Äôs 2 million token capability1.5 Flash is the newest addition to the Gemini model family and the fastest Gemini model served in the API. It‚Äôs optimized for high-volume, high-frequency tasks at scale, is more cost-efficient to serve and features our breakthrough long context window.While it‚Äôs a lighter weight model than 1.5 Pro, it‚Äôs highly capable of multimodal reasoning across vast amounts of information and delivers impressive quality for its size.The new Gemini 1.5 Flash model is optimized for speed and efficiency, is highly capable of multimodal reasoning and features our breakthrough long context window.1.5 Flash excels at summarization, chat applications, image and video captioning, data extraction from long documents and tables, and more. This is because it‚Äôs been trained by 1.5 Pro through a process called ‚Äúdistillation,‚Äù where the most essential knowledge and skills from a larger model are transferred to a smaller, more efficient model.Read more about 1.5 Flash in our updated Gemini 1.5 technical report, on the Gemini technology page, and learn about 1.5 Flash‚Äôs availability and pricing.Over the last few months, we‚Äôve significantly improved 1.5 Pro, our best model for general performance across a wide range of tasks.Beyond extending its context window to 2 million tokens, we‚Äôve enhanced its code generation, logical reasoning and planning, multi-turn conversation, and audio and image understanding through data and algorithmic advances. We see strong improvements on public and internal benchmarks for each of these tasks.1.5 Pro can now follow increasingly complex and nuanced instructions, including ones that specify product-level behavior involving role, format and style. We‚Äôve improved control over the model‚Äôs responses for specific use cases, like crafting the persona and response style of a chat agent or automating workflows through multiple function calls. And we‚Äôve enabled users to steer model behavior by setting system instructions.We added audio understanding in the Gemini API and Google AI Studio, so 1.5 Pro can now reason across image and audio for videos uploaded in Google AI Studio. And we‚Äôre now integrating 1.5 Pro into Google products, including Gemini Advanced and in Workspace apps.Read more about 1.5 Pro in our updated Gemini 1.5 technical report and on the Gemini technology page.Gemini Nano is expanding beyond text-only inputs to include images as well. Starting with Pixel, applications using Gemini Nano with Multimodality will be able to understand the world the way people do ‚Äî not just through text, but also through sight, sound and spoken language.Read more about Gemini 1.0 Nano on Android.Today, we‚Äôre also sharing a series of updates to Gemma, our family of open models built from the same research and technology used to create the Gemini models.We‚Äôre announcing Gemma 2, our next generation of open models for responsible AI innovation. Gemma 2 has a new architecture designed for breakthrough performance and efficiency, and will be available in new sizes.The Gemma family is also expanding with PaliGemma, our first vision-language model inspired by PaLI-3. And we‚Äôve upgraded our Responsible Generative AI Toolkit with LLM Comparator for evaluating the quality of model responses.Read more on the Developer blog.As part of Google DeepMind‚Äôs mission to build AI responsibly to benefit humanity, we‚Äôve always wanted to develop universal AI agents that can be helpful in everyday life. That‚Äôs why today, we‚Äôre sharing our progress in building the future of AI assistants with Project Astra (advanced seeing and talking responsive agent).To be truly useful, an agent needs to understand and respond to the complex and dynamic world just like people do ‚Äî and take in and remember what it sees and hears to understand context and take action. It also needs to be proactive, teachable and personal, so users can talk to it naturally and without lag or delay.While we‚Äôve made incredible progress developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Over the past few years, we‚Äôve been working to improve how our models perceive, reason and converse to make the pace and quality of interaction feel more natural.Building on Gemini, we‚Äôve developed prototype agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events, and caching this information for efficient recall.By leveraging our leading speech models, we also enhanced how they sound, giving the agents a wider range of intonations. These agents can better understand the context they‚Äôre being used in, and respond quickly, in conversation.With technology like this, it‚Äôs easy to envision a future where people could have an expert AI assistant by their side, through a phone or glasses. And some of these capabilities are coming to Google products, like the Gemini app and web experience, later this year.We‚Äôve made incredible progress so far with our family of Gemini models, and we‚Äôre always striving to advance the state-of-the-art even further. By investing in a relentless production line of innovation, we‚Äôre able to explore new ideas at the frontier, while also unlocking the possibility of new and exciting Gemini use cases.Learn more about Gemini and its capabilities. Your information will be used in accordance with Google‚Äôs privacy policy.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      Done. Just one step more.
    
      Check your inbox to confirm your subscription.
    You are already subscribed to our newsletter.
    You can also subscribe with a
    different email address
    
    .
    
  Let‚Äôs stay in touch. Get the latest news from Google in your inbox.
          Follow Us
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[We‚Äôre sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://stan-haochen.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://stan-haochen.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://stan-haochen.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[<h3>External Posts on Your al-folio¬†Blog</h3> <p>If you prefer publishing blog posts on medium.com or other external sources, starting version v0.5.0, <a href="https://github.com/alshedivat/al-folio">al-folio</a> lets you to display your external posts in the blog feed of your website!¬†üéâüéâ</p> <p>Configuring external sources of super simple. After upgrading to v0.5.0, just add the following section to your _config.yml:</p> <pre>external_sources:<br />  - name: medium.com  # name of the source (arbitrary string)<br />    rss_url: <a href="https://medium.com/@al-folio/feed">https://medium.com/@&lt;your-medium-username&gt;/feed</a></pre> <p>The example above adds your medium.com blog post feed as an external source. But you can add arbitrary RSS feeds as¬†sources.</p> <p>Any questions or suggestions? üëâ Start <a href="https://github.com/alshedivat/al-folio/discussions">a discussion on¬†GitHub</a>!</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b60a1d241a0a" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">On Optimization in Deep Learning</title><link href="https://stan-haochen.github.io/blog/2016/optimization-in-rl/" rel="alternate" type="text/html" title="On Optimization in Deep Learning"/><published>2016-09-07T00:00:00+00:00</published><updated>2016-09-07T00:00:00+00:00</updated><id>https://stan-haochen.github.io/blog/2016/optimization-in-rl</id><content type="html" xml:base="https://stan-haochen.github.io/blog/2016/optimization-in-rl/"><![CDATA[<p>This is an old post which may not fit into modern view. Some recent finding such as lottery ticket theory is not covered in this post.</p> <p>There are at least exponentially many global minima for a neural net. Since permuating the nodes in one layer does not change the loss. Finding such points is not easy. Before certain techniques such as momentum came out, those nets were considered impossible to learn.</p> <p>Thanks to the constantly envolving hardwares and libraries, we do not have to worry about training time <em>that much</em> at least for convnets. Empirically, the non-convexity of neural nets seems not to be an issue. In practice, SGD works pretty well in optimizing very large networks even though the problem is proved to be NP-hard. However, researchers never stop studying the loss surface of deep neural nets and searching for better optimization strategies.</p> <p><a href="https://arxiv.org/abs/1605.07110">This paper</a> has been renewed on ArXiv recently, which leads me to <a href="https://news.ycombinator.com/item?id=11765111">this discussion</a>. Following are what I find interesting.</p> <h2 id="why-sgd-works">Why SGD works?</h2> <p>[Choromaska et al, AISTATS‚Äô15] (also [Dauphin et al, ICML‚Äô15] use tools from Statistical Physics to explain the behavior of stochastic gradient methods when training deep neural networks. This offers a macroscopic explanation of why SGD ‚Äúworks‚Äù, and gives a characterization of the network depth. The model is strongly simplified, and convolution is not considered.</p> <h3 id="saddle-points">Saddle points</h3> <p>We start from discussing saddle points, the vast majority of critical points on the error surfaces of neural networks.</p> <blockquote> <p>Here we argue, ‚Ä¶ that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum.</p> <p>‚Äì <cite> Dauphin et al, <a href="http://arxiv.org/abs/1406.2572">Identifying and attacking the saddle point problem in high-dimensional non-convex optimization</a> </cite></p> </blockquote> <p>The authors introduce saddle-free Newton method which requires the estimation of Hessian. They connect the loss function of a deep net to a high-dimensional Gaussian random field. They show that critical points with high training error are exponentially likely to be saddle points with many negative directions, and all local minima are likely to have error that is very close to that of the global minimum. (Described in <a href="https://arxiv.org/abs/1611.01838">Entropy-SGD: Biasing Gradient Descent Into Wide Valleys</a>.)</p> <p>The convergence of gradient descent is affected by the proliferation of saddle points surrounded by high error plateaus ‚Äî as opposed to multiple local minima.</p> <blockquote> <p>The time spent by diffusion is inversely proportional to the smallest negative eigenvalue of the Hessian at a saddle point</p> <p>‚Äì <cite>Kramer‚Äôs law</cite></p> </blockquote> <blockquote> <p>It is believed that for many problems including learning deep nets, almost all local minimum have very similar function value to the global optimum, and hence finding a local minimum is good enough.</p> <p>‚Äì <cite> Rong Ge, <a href="http://www.offconvex.org/2016/03/22/saddlepoints/">Escaping from Saddle Points</a> </cite></p> </blockquote> <p>As the model grows deeper, local minima have loss closer to global minima. On the other hand, we do not care about global minimum because it often leads to overfitting.</p> <p>Saddle points exist along the paths between local minima, most objective functions have exponentially many of those. However, first order optimization algorithms may get stuck at saddle points. Strict saddle points can be escaped and global minima can be achieved in polynomial time (<a href="http://arxiv.org/abs/1503.02101">Ge et al., 2015</a>). Stochastic gradient introduces noise and help to push the current point away from saddle points.</p> <p>Non-convex problems can have ‚Äò‚Äòdegenerate saddle points‚Äô‚Äô, whose Hessian is p.s.d. and have 0 eigenvalues. The performance of SGD on these kind of tasks is still not well studied.</p> <p>To conclude this part, AFAIK, we should care more about escaping from saddle point. And gradient based methods can do a better job than second-order methods in practice.</p> <h3 id="spin-glass-hamiltonian">Spin-glass Hamiltonian</h3> <p>See <a href="https://charlesmartin14.wordpress.com/2015/03/25/why-does-deep-learning-work/">Charles Martin: Why Does Deep Learning Works?</a> Both papers mentioned above use ideas from statistical physics and spin-glass models.</p> <table> <tbody> <tr> <td>Statistical physicists refer to $H_x(y)\equiv-\ln p(y</td> <td>x)$ as the <strong>Hamiltonian</strong>, quantifying the energy of $y$ given the parameter $x$. And $\mu\equiv -\ln p$ as <strong>self-information</strong>. We can rewrite Bayes‚Äô formula as:</td> </tr> </tbody> </table> \[p(y) = \sigma(-H(y)-\mu)\] <p>We can see the features yield by a neural net as Hamiltonian and the softmax computes the classification probability.</p> <blockquote> <p>The long-term behavior of certain neural network models are governed by the statistical mechanism of infinite-range Ising spin-glass Hamiltonians</p> <p>‚Äì <cite> LeCun et. al., <a href="https://arxiv.org/abs/1412.0233">The Loss Surfaces of Multilayer Networks, 2015</a> </cite></p> </blockquote> <p>In this paper, he tries to explain the optimization paradigm with spin-glass theory.</p> <h3 id="implicit-bias-in-sgd">Implicit Bias in SGD</h3> <ul> <li><a href="https://arxiv.org/abs/1611.01838">Chaudhari</a> proposed a surrogate loss that explicitly biases SGD dynamics towards flat local minima. The corresponding algorithm relates closely to stochastic gradient Langevin dynamics.</li> <li>Another interpretation is that SGD performs Variational Inference (VI).</li> </ul> <h2 id="what-does-the-minima-look-like">What does the minima look like?</h2> <p>Take for example the concept of mode connectivity (<a href="https://arxiv.org/abs/1802.10026">Garipov et al, 2018</a>): it seems that the modes found by SGD using different random seeds are not just isolated basins, but they are connected by smooth valleys along which the training and test error are low.</p> <h3 id="no-poor-local-minima">No poor local minima</h3> <p><a href="https://arxiv.org/abs/1412.6544">Research at Google and Stanford</a> confirms that the Deep Learning Energy Landscapes appear to be roughly convex. A bolder hypothesis is that deep networks are spin funnels. And as the net gets larger, the funnel gets sharper. If this is true, our major concern should be to avoid over-training rather than the convexity of the network.</p> <p>Finally we arrive at the paper itself. Nets are optimized well by local gradient methods and seems not to be affected by local minima. The author claims that every local minimum is a global minimum and ‚Äúbad‚Äù saddle points (degenerated ones) exists for deeper nets. Thm 2.3 gives clear result on linear networks.</p> <p>The main result Thm 3.2 generalizes <a href="https://arxiv.org/abs/1412.0233">Choromanska et al, 2015</a>‚Äôs idea for nonlinear network relies on 4 (seemingly strong) assumptions:</p> <ol> <li>The dimensionality of the output is smaller than the input.</li> <li>The inputs are random and decorrelated.</li> <li>A connection in the network is activated or not is random with the same probability of success across the network. (ReLU thresholding happens randomly.)</li> <li>The network activations are independent of the input, the weights and each other.</li> </ol> <p>They relax the majority of the asssumptions, which is very promising, but leave a weaker condition A1u-m and A5u-m (<a href="https://www.reddit.com/r/MachineLearning/comments/4ktqeu/160507110_deep_learning_without_poor_local_minima/">from reddit post</a>).</p> <p>Recently DeepMind came up with <a href="https://arxiv.org/abs/1611.06310">another paper</a> claiming the assumptions are too strong for real data. And devised counter examples with finite datatets for rectified MLPs. For finite sized models/datasets, one does not have a globally good behavior of learning regardless of the model size.</p> <p>Even though deep learning energy landscapes appear to be roughly convex, or as this post referred to, local minimal free, a deep model has to include more engineering details to aid its convergence. Problems such as covariance shift and overfitting still have to be handled by engineering techniques.</p> <h3 id="arriving-on-flatter-minima">Arriving on flatter minima</h3> <blockquote> <p>large-batch methods tend to converge to sharp minimizers of the training and testing functions ‚Äì and that sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation.</p> <p>‚Äì <cite> <a href="https://stanstarks.github.io/tw5/#On%20Large-Batch%20Training%20for%20Deep%20Learning%3A%20Generalization%20Gap%20and%20Sharp%20Minima">On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</a> </cite></p> </blockquote> <ul> <li><a href="https://arxiv.org/abs/1802.06175">An Alternative View: When Does SGD Escape Local Minima?</a></li> </ul> <h2 id="should-2-nd-order-methods-ever-work">Should 2-nd order methods ever work?</h2> <p>Basiclly no. Because the Hessian vector product require very low variance estimation, which leads to batch size larger than 1000. But <a href="https://www.reddit.com/r/MachineLearning/comments/599wbr/project_i_accidentally_wrote_a_quasinewton_lbfgs/">some rare cases</a> happen when 2nd order methods with small batch size works.</p> <h2 id="gradient-starvation">Gradient Starvation</h2> <ul> <li><a href="https://arxiv.org/abs/1809.06848">On the Learning Dynamics of Deep Neural Networks</a> <ul> <li>Some features will dominate the gradient and sheding other equally important features.</li> </ul> </li> </ul>]]></content><author><name>Hao Chen</name></author><category term="distill"/><category term="formatting"/><summary type="html"><![CDATA[An old post about no local minima in Deep Learning.]]></summary></entry></feed>