---
---

@inproceedings{zhang2025pomato,
  abbr={ICCV},
  title={POMATO: Marrying Pointmap Matching with Temporal Motion for Dynamic 3D Reconstruction},
  author={Zhang, Songyan and Ge, Yongtao and Tian, Jinyuan and Xu, Guangkai and Chen, Hao and Lv, Chen and Shen, Chunhua},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  year={2025},
  code = {https://github.com/wyddmw/POMATO},
  preview={pomato.png},
  selected={true}
}

@inproceedings{gao2025surfacesplat,
  abbr={ICCV},
  title={SurfaceSplat: Connecting Surface Reconstruction and Gaussian Splatting},
  author={Gao, Zihui and Bian, Jia-Wang and Lin, Guosheng and Chen, Hao and Shen, Chunhua},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  year={2025},
  html = {https://openreview.net/forum?id=hDIP1pQlQK},
  preview={surfacesplat.png}
}

@inproceedings{liu2025unified,
  abbr={ICCV},
  title={Unified Open-World Segmentation with Multi-Modal Prompts},
  author = {Liu, Yang and Yin, Yufei and Jing, Chenchen and Zhu, Muzhi and Chen, Hao and Xi, Yuling and Feng, Bo and Wang, Hao and Li, Shiyu and Shen, Chunhua},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  year={2025},
  html = {hhttps://openreview.net/forum?id=tpbvNY1nHL},
  preview={cosine.png}
}

@inproceedings{ge2025gvm,
  abbr={SIGGRAPH},
  author = {Ge, Yongtao and Xie, Kangyang and Xu, Guangkai and Ke, Li and Liu, Mingyu and Huang, Longtao and Xue, Hui and Chen, Hao and Shen, Chunhua},
  title = {Generative Video Matting},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3721238.3730642},
  doi = {10.1145/3721238.3730642},
  booktitle = {Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers},
  year={2025},
  series = {SIGGRAPH Conference Papers},
  code = {https://github.com/aim-uofa/GVM},
  preview={gvm.png}
}

@inproceedings{zhuSegAgentExploringPixel2025,
  abbr={CVPR},
  title = {{{SegAgent}}: {{Exploring Pixel Understanding Capabilities}} in {{MLLMs}} by {{Imitating Human Annotator Trajectories}}},
  shorttitle = {{{SegAgent}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Zhu, Muzhi and Tian, Yuzhuo and Chen, Hao and Zhou, Chunluan and Guo, Qingpei and Liu, Yang and Yang, Ming and Shen, Chunhua},
  year = {2025},
  pages = {3686--3696},
  urldate = {2025-08-20},
  preview = {segagent.png},
  code = {https://github.com/aim-uofa/SegAgent},
  website = {https://aim-uofa.github.io/SegAgent/},
  selected={true}
}

@inproceedings{boRevisitingConvolutionArchitecture2024,
  abbr={ICLR},
  title = {Revisiting {{Convolution Architecture}} in the {{Realm}} of {{DNA Foundation Models}}},
  booktitle = {The {{Thirteenth International Conference}} on {{Learning Representations}}},
  author = {Bo, Yu and Mao, Weian and Shao, Yanjun and Bai, Weiqiang and Ye, Peng and Ma, Xinzhu and Zhao, Junbo and Chen, Hao and Shen, Chunhua},
  year = {2025},
  preview = {convnova.jpg},
  code={https://github.com/aim-uofa/ConvNova},
  html={https://openreview.net/forum?id=B07dLVWLyD}
}

@inproceedings{chenPerturboLLaVAReducingMultimodal2024,
  abbr={ICLR},
  title = {{{PerturboLLaVA}}: {{Reducing Multimodal Hallucinations}} with {{Perturbative Visual Training}}},
  shorttitle = {{{PerturboLLaVA}}},
  booktitle = {The {{Thirteenth International Conference}} on {{Learning Representations}}},
  author = {Chen, Cong and Liu, Mingyu and Jing, Chenchen and Zhou, Yizhou and Rao, Fengyun and Chen, Hao and Zhang, Bo and Shen, Chunhua},
  year = {2025},
  code={https://github.com/aim-uofa/PerturboLLaVA},
  preview = {perturbollava.png}
}

@inproceedings{jiaoBoltzmannAlignedInverseFolding2024,
  abbr={ICLR},
  title = {Boltzmann-{{Aligned Inverse Folding Model}} as a {{Predictor}} of {{Mutational Effects}} on {{Protein-Protein Interactions}}},
  booktitle = {The {{Thirteenth International Conference}} on {{Learning Representations}}},
  author = {Jiao, Xiaoran and Mao, Weian and Jin, Wengong and Yang, Peiyuan and Chen, Hao and Shen, Chunhua},
  year = {2025},
  abstract = {Predicting the change in binding free energy (\${\textbackslash}Delta {\textbackslash}Delta G\$) is crucial for understanding and modulating protein-protein interactions, which are critical in drug design. Due to the scarcity of experimental \${\textbackslash}Delta{\textbackslash}Delta G\$ data, existing methods focus on pre-training, while alignment receives less attention. In this work, we propose the Boltzmann Alignment technique to transfer knowledge from pre-trained inverse folding models to \${\textbackslash}Delta{\textbackslash}Delta G\$ prediction. We begin by analyzing the thermodynamic definition of \${\textbackslash}Delta{\textbackslash}Delta G\$ and introducing the Boltzmann distribution to connect energy with protein conformational distribution. However, the protein conformational distribution is intractable; therefore, we employ Bayes' theorem to circumvent direct estimation and instead utilize the log-likelihood provided by protein inverse folding models for \${\textbackslash}Delta{\textbackslash}Delta G\$ estimation. Compared to previous inverse folding-based methods, our method explicitly accounts for the unbound state of protein complex in the \${\textbackslash}Delta {\textbackslash}Delta G\$ thermodynamic cycle, introducing a physical inductive bias and achieving both supervised and unsupervised state-of-the-art (SoTA) performance. Experimental results on SKEMPI v2 indicate that our method achieves Spearman coefficients of 0.3201 (unsupervised) and 0.5134 (supervised) on SKEMPI v2, significantly surpassing the previously reported SoTA values of 0.2632 and 0.4324, respectively. Futhermore, we demonstrate the capability of our method on binding energy prediction, protein-protein docking and antibody optimization tasks.},
  code={https://github.com/aim-uofa/BA-DDG},
  preview = {baddg.png}
}

@inproceedings{zhaoMovieDreamerHierarchicalGeneration2024,
  abbr={ICLR},
  title = {{{MovieDreamer}}: {{Hierarchical Generation}} for {{Coherent Long Visual Sequences}}},
  shorttitle = {{{MovieDreamer}}},
  booktitle = {The {{Thirteenth International Conference}} on {{Learning Representations}}},
  author = {Zhao, Canyu and Liu, Mingyu and Wang, Wen and Chen, Weihua and Wang, Fan and Chen, Hao and Zhang, Bo and Shen, Chunhua},
  year = {2025},
  code={https://github.com/aim-uofa/MovieDreamer},
  website={https://aim-uofa.github.io/MovieDreamer/},
  preview = {moviedreamer.png}
}

@inproceedings{wangFramerInteractiveFrame2024,
  abbr={ICLR},
  title = {Framer: {{Interactive Frame Interpolation}}},
  shorttitle = {Framer},
  booktitle = {The {{Thirteenth International Conference}} on {{Learning Representations}}},
  author = {Wang, Wen and Wang, Qiuyu and Zheng, Kecheng and Ouyang, Hao and Chen, Zhekai and Gong, Biao and Chen, Hao and Shen, Yujun and Shen, Chunhua},
  year = {2025},
  code={https://github.com/aim-uofa/Framer},
  website={https://aim-uofa.github.io/Framer/},
  preview={framer.png},
  video={https://youtu.be/4MPGKgn7jRc},
  selected={true}
}

@inproceedings{zhuGenerativeActiveLearning2024,
  abbr={ICML},
  title = {Generative {{Active Learning}} for {{Long-tailed Instance Segmentation}}},
  booktitle = {Forty-First {{International Conference}} on {{Machine Learning}}},
  author = {Zhu, Muzhi and Fan, Chengxiang and Chen, Hao and Liu, Yang and Mao, Weian and Xu, Xiaogang and Shen, Chunhua},
  year = {2024},
  urldate = {2025-02-17},
  abstract = {Recently, large-scale language-image generative models have gained widespread attention and many works have utilized generated data from these models to further enhance the performance of perception tasks. However, not all generated data can positively impact downstream models, and these methods do not thoroughly explore how to better select and utilize generated data. On the other hand, there is still a lack of research oriented towards active learning on generated data. In this paper, we explore how to perform active learning specifically for generated data in the long-tailed instance segmentation task. Subsequently, we propose BSGAL, a new algorithm that estimates the contribution of the current batch-generated data based on gradient cache. BSGAL is meticulously designed to cater for unlimited generated data and complex downstream segmentation tasks. BSGAL outperforms the baseline approach and effectually improves the performance of long-tailed segmentation.},
  code={https://github.com/aim-uofa/DiverGen/tree/main/BSGAL},
  preview={bsgal.png},
  selected={true}
}

@inproceedings{liuFloatingAnchorDiffusion2024,
  abbr={ICML},
  title = {Floating {{Anchor Diffusion Model}} for {{Multi-motif Scaffolding}}},
  booktitle = {Forty-First {{International Conference}} on {{Machine Learning}}},
  author = {Liu, Ke and Mao, Weian and Shen, Shuaike and Jiao, Xiaoran and Sun, Zheng and Chen, Hao and Shen, Chunhua},
  year = {2024},
  code = {https://github.com/aim-uofa/FADiff}
}

@article{huMetric3DV2Versatile2024,
  abbr={TPAMI},
  title = {{{Metric3D}} v2: {{A Versatile Monocular Geometric Foundation Model}} for {{Zero-Shot Metric Depth}} and {{Surface Normal Estimation}}},
  shorttitle = {{{Metric3D}} V2},
  author = {Hu, Mu and Yin, Wei and Zhang, Chi and Cai, Zhipeng and Long, Xiaoxiao and Chen, Hao and Wang, Kaixuan and Yu, Gang and Shen, Chunhua and Shen, Shaojie},
  year = {2024},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {46},
  number = {12},
  pages = {10579--10596},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2024.3444912},
  abstract = {We introduce Metric3D v2, a geometric foundation model designed for zero-shot metric depth and surface normal estimation from single images, critical for accurate 3D recovery. Depth and normal estimation, though complementary, present distinct challenges. State-of-the-art monocular depth methods achieve zero-shot generalization through affine-invariant depths, but fail to recover real-world metric scale. Conversely, current normal estimation techniques struggle with zero-shot performance due to insufficient labeled data. We propose targeted solutions for both metric depth and normal estimation. For metric depth, we present a canonical camera space transformation module that resolves metric ambiguity across various camera models and large-scale datasets, which can be easily integrated into existing monocular models. For surface normal estimation, we introduce a joint depth-normal optimization module that leverages diverse data from metric depth, allowing normal estimators to improve beyond traditional labels. Our model, trained on over 16 million images from thousands of camera models with varied annotations, excels in zero-shot generalization to new camera settings. As shown in Fig. 1, It ranks the 1st in multiple zero-shot and standard benchmarks for metric depth and surface normal prediction. Our method enables the accurate recovery of metric 3D structures on randomly collected internet images, paving the way for plausible single-image metrology. Our model also relieves the scale drift issues of monocular-SLAM (Fig. 3), leading to high-quality metric scale dense mapping. Such applications highlight the versatility of Metric3D v2 models as geometric foundation models.},
  website = {https://jugghm.github.io/Metric3Dv2/},
  preview = {metric3dv2.png},
  video = {https://jugghm.github.io/Metric3Dv2/resource_new/media/demo_full.mp4}
}

@inproceedings{heDiffCalibReformulatingMonocular2024,
  abbr={AAAI},
  title = {{{DiffCalib}}: {{Reformulating Monocular Camera Calibration}} as {{Diffusion-Based Dense Incident Map Generation}}},
  shorttitle = {{{DiffCalib}}},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {He, Xiankang and Xu, Guangkai and Zhang, Bo and Chen, Hao and Cui, Ying and Guo, Dongyan},
  year = {2024},
  code={https://github.com/zjutcvg/DiffCalib},
  preview = {diffcalib.jpg}
}

@article{jingRetrievalAugmentedPrimitiveRepresentations2024,
  abbr={AAAI},
  title = {Retrieval-{{Augmented Primitive Representations}} for {{Compositional Zero-Shot Learning}}},
  author = {Jing, Chenchen and Li, Yukun and Chen, Hao and Shen, Chunhua},
  year = {2024},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {38},
  number = {3},
  pages = {2652--2660},
  issn = {2374-3468},
  doi = {10.1609/aaai.v38i3.28043},
  code={https://github.com/jingchenchen/RAPR}
}

@inproceedings{liuSimpleImageSegmentation2025,
  abbr={NeurIPS},
  title = {A {{Simple Image Segmentation Framework}} via {{In-Context Examples}}},
  author = {Liu, Yang and Jing, Chenchen and Li, Hengtao and Zhu, Muzhi and Chen, Hao and Wang, Xinlong and Shen, Chunhua},
  year = {2024},
  journal = {Advances in Neural Information Processing Systems},
  volume = {37},
  pages = {25095--25119},
  code={https://github.com/aim-uofa/SINE},
  preview={sine.png}
}

@inproceedings{chenFreeComposeGenericZeroShot2025,
  abbr={ECCV},
  title = {{{FreeCompose}}: {{Generic Zero-Shot Image Composition}} with~{{Diffusion Prior}}},
  shorttitle = {{{FreeCompose}}},
  booktitle = {The 17th {{European Conference}} on {{Computer Vision ECCV}} 2024},
  author = {Chen, Zhekai and Wang, Wen and Yang, Zhen and Yuan, Zeqing and Chen, Hao and Shen, Chunhua},
  year = {2024},
  code={https://github.com/aim-uofa/FreeCompose},
  preview = {freecompose.png}
}

@inproceedings{dingFreeCustomTuningFreeCustomized2024,
  abbr={CVPR},
  title = {{{FreeCustom}}: {{Tuning-Free Customized Image Generation}} for {{Multi-Concept Composition}}},
  shorttitle = {{{FreeCustom}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Ding, Ganggui and Zhao, Canyu and Wang, Wen and Yang, Zhen and Liu, Zide and Chen, Hao and Shen, Chunhua},
  year = {2024},
  code={https://github.com/aim-uofa/FreeCustom},
  website={https://aim-uofa.github.io/FreeCustom/},
  preview = {freecustom.png}
}

@inproceedings{fanDiverGenImprovingInstance2024,
  abbr={CVPR},
  title = {{{DiverGen}}: {{Improving Instance Segmentation}} by {{Learning Wider Data Distribution}} with {{More Diverse Generative Data}}},
  shorttitle = {{{DiverGen}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Fan, Chengxiang and Zhu, Muzhi and Chen, Hao and Liu, Yang and Wu, Weijia and Zhang, Huaqi and Shen, Chunhua},
  year = {2024},
  code={https://github.com/aim-uofa/DiverGen/tree/main/DiverGen},
  preview = {divergen.png}
}

@inproceedings{liuMatcherSegmentAnything2023,
  abbr={ICLR},
  title = {Matcher: {{Segment Anything}} with {{One Shot Using All-Purpose Feature Matching}}},
  shorttitle = {Matcher},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Liu, Yang and Zhu, Muzhi and Li, Hengtao and Chen, Hao and Wang, Xinlong and Shen, Chunhua},
  year = {2024},
  code = {https://github.com/aim-uofa/Matcher},
  preview = {matcher.png},
  selected={true}
}

@article{wangAutoStoryGeneratingDiverse2024,
  abbr={IJCV},
  title = {{{AutoStory}}: {{Generating Diverse Storytelling Images}} with {{Minimal Human Efforts}}},
  shorttitle = {{{AutoStory}}},
  author = {Wang, Wen and Zhao, Canyu and Chen, Hao and Chen, Zhekai and Zheng, Kecheng and Shen, Chunhua},
  year = {2024},
  journal = {International Journal of Computer Vision},
  issn = {1573-1405},
  doi = {10.1007/s11263-024-02309-y},
  website = {https://aim-uofa.github.io/AutoStory/},
  code = {https://github.com/aim-uofa/AutoStory},
  preview={autostory.png}
}

@inproceedings{maoNovoProteinDesign2023,
  abbr={ICLR},
  title = {De Novo {{Protein Design Using Geometric Vector Field Networks}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Mao, Weian and Zhu, Muzhi and Sun, Zheng and Shen, Shuaike and Wu, Lin Yuanbo and Chen, Hao and Shen, Chunhua},
  year = {2024},
  urldate = {2025-02-17},
  abstract = {Advances like protein diffusion have marked revolutionary progress in \${\textbackslash}textit\{de novo\}\$ protein design, a central topic in life science. These methods typically depend on protein structure encoders to model residue backbone frames, where atoms do not exist. Most prior encoders rely on atom-wise features, such as angles and distances between atoms, which are not available in this context. Only a few basic encoders, like IPA, have been proposed for this scenario, exposing the frame modeling as a bottleneck. In this work, we introduce the Vector Field Network (VFN), that enables network layers to perform learnable vector computations between coordinates of frame-anchored virtual atoms, thus achieving a higher capability for modeling frames. The vector computation operates in a manner similar to a linear layer, with each input channel receiving 3D virtual atom coordinates instead of scalar values. The multiple feature vectors output by the vector computation are then used to update the residue representations and virtual atom coordinates via attention aggregation. Remarkably, VFN also excels in modeling both frames and atoms, as the real atoms can be treated as the virtual atoms for modeling, positioning VFN as a potential \${\textbackslash}textit\{universal encoder\}\$. In protein diffusion (frame modeling), VFN exhibits a impressive performance advantage over IPA, excelling in terms of both designability (\${\textbackslash}textbf\{67.04\}\${\textbackslash}\% vs. 53.58{\textbackslash}\%) and diversity (\${\textbackslash}textbf\{66.54\}\${\textbackslash}\% vs. 51.98{\textbackslash}\%). In inverse folding(frame and atom modeling), VFN outperforms the previous SoTA model, PiFold (\${\textbackslash}textbf\{54.7\}\${\textbackslash}\% vs. 51.66{\textbackslash}\%), on sequence recovery rate; we also propose a method of equipping VFN with the ESM model, which significantly surpasses the previous ESM-based SoTA (\${\textbackslash}textbf\{62.67\}\${\textbackslash}\% vs. 55.65{\textbackslash}\%), LM-Design, by a substantial margin. Code is available at https://github.com/aim-uofa/VFN},
  langid = {english},
  file = {C:\Users\stanz\Documents\Papers\Zotero\Projects\Hao\2023\De novo Protein Design Using Geometric Vector Field Networks.pdf}
}

@article{wuDatasetDMSynthesizingData2023,
  abbr={NeurIPS},
  title = {{{DatasetDM}}: {{Synthesizing Data}} with {{Perception Annotations Using Diffusion Models}}},
  shorttitle = {{{DatasetDM}}},
  author = {Wu, Weijia and Zhao, Yuzhong and Chen, Hao and Gu, Yuchao and Zhao, Rui and He, Yefei and Zhou, Hong and Shou, Mike Zheng and Shen, Chunhua},
  year = {2023},
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {54683--54695}
}

@inproceedings{liLearningFuseMonocular2023,
  abbr={CVPR},
  title = {Learning {{To Fuse Monocular}} and {{Multi-View Cues}} for {{Multi-Frame Depth Estimation}} in {{Dynamic Scenes}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Li, Rui and Gong, Dong and Yin, Wei and Chen, Hao and Zhu, Yu and Wang, Kaixuan and Chen, Xiaozhi and Sun, Jinqiu and Zhang, Yanning},
  year = {2023},
  pages = {21539--21548}
}

@article{maLSSInstImprovingGeometric2024,
  abbr={3DV},
  title = {{{LSSInst}}: {{Improving Geometric Modeling}} in {{LSS-Based BEV Perception}} with {{Instance Representation}}},
  shorttitle = {{{LSSInst}}},
  author = {Ma, Weijie and Jiang, Jingwei and Yang, Yang and Chen, Zehui and Chen, Hao},
  year = {2024}
}

@inproceedings{wangLearningConditionalAttributes2023,
  abbr={CVPR},
  title = {Learning {{Conditional Attributes}} for {{Compositional Zero-Shot Learning}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Wang, Qingsheng and Liu, Lingqiao and Jing, Chenchen and Chen, Hao and Liang, Guoqiang and Wang, Peng and Shen, Chunhua},
  year = {2023},
  pages = {11197--11206}
}

@article{xiDynamicFeatureInteraction2023,
  title = {A {{Dynamic Feature Interaction Framework}} for {{Multi-task Visual Perception}}},
  author = {Xi, Yuling and Chen, Hao and Wang, Ning and Wang, Peng and Zhang, Yanning and Shen, Chunhua and Liu, Yifan},
  year = {2023},
  month = nov,
  journal = {International Journal of Computer Vision},
  volume = {131},
  number = {11},
  pages = {2977--2993},
  issn = {1573-1405},
  doi = {10.1007/s11263-023-01835-5},
  urldate = {2025-02-17},
  abstract = {Multi-task visual perception has a wide range of applications in scene understanding such as autonomous driving. In this work, we devise an efficient unified framework to solve multiple common perception tasks, including instance segmentation, semantic segmentation, monocular 3D detection, and depth estimation. Simply sharing the same visual feature representations for these tasks impairs the performance of tasks, while independent task-specific feature extractors lead to parameter redundancy and latency. Thus, we design two feature-merge branches to learn feature basis, which can be useful to, and thus shared by, multiple perception tasks. Then, each task takes the corresponding feature basis as the input of the prediction task head to fulfill a specific task. In particular, one feature merge branch is designed for instance-level recognition the other for dense predictions. To enhance inter-branch communication, the instance branch passes pixel-wise spatial information of each instance to the dense branch using efficient dynamic convolution weighting. Moreover, a simple but effective dynamic routing mechanism is proposed to isolate task-specific features and leverage common properties among tasks. Our proposed framework, termed D2BNet, demonstrates a unique approach to parameter-efficient predictions for multi-task perception. In addition, as tasks benefit from co-training with each other, our solution achieves on par results on partially labeled settings on nuScenes and outperforms previous works for 3D detection and depth estimation on the Cityscapes dataset with full supervision.},
  langid = {english},
  keywords = {3D object detection,Artificial Intelligence,Depth estimation,Dynamic routing,Multi-task perception,Panoptic segmentation},
  file = {C:\Users\stanz\Documents\Papers\Zotero\Projects\Hao\2023\A Dynamic Feature Interaction Framework for Multi-task Visual Perception.pdf}
}

@inproceedings{xuFrozenReconPosefree3D2023,
  title = {{{FrozenRecon}}: {{Pose-free 3D Scene Reconstruction}} with {{Frozen Depth Models}}},
  shorttitle = {{{FrozenRecon}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Xu, Guangkai and Yin, Wei and Chen, Hao and Shen, Chunhua and Cheng, Kai and Zhao, Feng},
  year = {2023},
  pages = {9310--9320},
  urldate = {2025-02-17},
  langid = {english},
  selected={true},
  file = {C:\Users\stanz\Documents\Papers\Zotero\Projects\Hao\2023\FrozenRecon Pose-free 3D Scene Reconstruction with Frozen Depth Models.pdf}
}

@inproceedings{xuWhatMattersWhen2024,
  title = {What {{Matters When Repurposing Diffusion Models}} for {{General Dense Perception Tasks}}?},
  booktitle = {The {{Thirteenth International Conference}} on {{Learning Representations}}},
  author = {Xu, Guangkai and Ge, Yongtao and Liu, Mingyu and Fan, Chengxiang and Xie, Kangyang and Zhao, Zhiyue and Chen, Hao and Shen, Chunhua},
  year = {2024},
  month = oct,
  urldate = {2025-02-17},
  abstract = {Extensive pre-training with large data is indispensable for downstream geometry and semantic visual perception tasks. Thanks to large-scale text-to-image (T2I) pretraining, recent works show promising results by simply fine-tuning T2I diffusion models for a few dense perception tasks. However, several crucial design decisions in this process still lack comprehensive justification, encompassing the necessity of the multi-step diffusion mechanism, training strategy, inference ensemble strategy, and fine-tuning data quality. In this work, we conduct a thorough investigation into critical factors that affect transfer efficiency and performance when using diffusion priors. Our key findings are: 1) High-quality fine-tuning data is paramount for both semantic and geometry perception tasks. 2) As a special case of the diffusion scheduler by setting its hyper-parameters, the multi-step generation can be simplified to a one-step fine-tuning paradigm without any loss of performance, while significantly speeding up inference. 3) Apart from fine-tuning the diffusion model with only latent space supervision, task-specific supervision can be beneficial to enhance fine-grained details. These observations culminate in the development of GenPercept, an effective deterministic one-step fine-tuning paradigm tailored for dense visual perception tasks exploiting diffusion priors. Different from the previous multi-step methods, our paradigm offers a much faster inference speed, and can be seamlessly integrated with customized perception decoders and loss functions for task-specific supervision, which can be critical for improving the fine-grained details of predictions. Comprehensive experiments on a diverse set of dense visual perceptual tasks, including monocular depth estimation, surface normal estimation, image segmentation, and matting, are performed to demonstrate the remarkable adaptability and effectiveness of our proposed method. Code: https://github.com/aim-uofa/GenPercept},
  langid = {english},
  selected={true},
  file = {C\:\\Users\\stanz\\Documents\\Papers\\Zotero\\Generative Models\\Gen4Perception\\2024\\What Matters When Repurposing Diffusion Models for General Dense Perception Tasks.pdf;C\:\\Users\\stanz\\Documents\\Papers\\Zotero\\Projects\\Hao\\2024\\What Matters When Repurposing Diffusion Models for General Dense Perception Tasks.pdf}
}

@inproceedings{yangObjectAwareInversionReassembly2023,
  title = {Object-{{Aware Inversion}} and {{Reassembly}} for {{Image Editing}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Yang, Zhen and Ding, Ganggui and Wang, Wen and Chen, Hao and Zhuang, Bohan and Shen, Chunhua},
  year = {2023},
  month = oct,
  urldate = {2025-02-17},
  abstract = {Diffusion-based image editing methods have achieved remarkable advances in text-driven image editing. The editing task aims to convert an input image with the original text prompt into the desired image that is well-aligned with the target text prompt. By comparing the original and target prompts, we can obtain numerous editing pairs, each comprising an object and its corresponding editing target. To allow editability while maintaining fidelity to the input image, existing editing methods typically involve a fixed number of inversion steps that project the whole input image to its noisier latent representation, followed by a denoising process guided by the target prompt. However, we find that the optimal number of inversion steps for achieving ideal editing results varies significantly among different editing pairs, owing to varying editing difficulties. Therefore, the current literature, which relies on a fixed number of inversion steps, produces sub-optimal generation quality, especially when handling multiple editing pairs in a natural image. To this end, we propose a new image editing paradigm, dubbed Object-aware Inversion and Reassembly (OIR), to enable object-level fine-grained editing. Specifically, we design a new search metric, which determines the optimal inversion steps for each editing pair, by jointly considering the editability of the target and the fidelity of the non-editing region. We use our search metric to find the optimal inversion step for each editing pair when editing an image. We then edit these editing pairs separately to avoid {\textbackslash}concept. Subsequently, we propose an additional reassembly step to seamlessly integrate the respective editing results and the non-editing region to obtain the final edited image. To systematically evaluate the effectiveness of our method, we collect two datasets called OIRBench for benchmarking single- and multi-object editing, respectively. Experiments demonstrate that our method achieves superior performance in editing object shapes, colors, materials, categories, {\textbackslash}textit\{etc.\}, especially in multi-object editing scenarios. The project page can be found in https://aim-uofa.github.io/OIR-Diffusion/.},
  langid = {english},
  file = {C:\Users\stanz\Documents\Papers\Zotero\Projects\Hao\2023\Object-Aware Inversion and Reassembly for Image Editing.pdf}
}

@inproceedings{yingCTVISConsistentTraining2023,
  title = {{{CTVIS}}: {{Consistent Training}} for {{Online Video Instance Segmentation}}},
  shorttitle = {{{CTVIS}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Ying, Kaining and Zhong, Qing and Mao, Weian and Wang, Zhenhua and Chen, Hao and Wu, Lin Yuanbo and Liu, Yifan and Fan, Chengxiang and Zhuge, Yunzhi and Shen, Chunhua},
  year = {2023},
  pages = {899--908},
  urldate = {2025-02-17},
  langid = {english},
  file = {C:\Users\stanz\Documents\Papers\Zotero\Projects\Hao\2023\CTVIS Consistent Training for Online Video Instance Segmentation.pdf}
}

@inproceedings{yinMetric3DZeroshotMetric2023,
  abbr={ICCV},
  title = {{{Metric3D}}: {{Towards Zero-shot Metric 3D Prediction}} from {{A Single Image}}},
  shorttitle = {{{Metric3D}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Yin, Wei and Zhang, Chi and Chen, Hao and Cai, Zhipeng and Yu, Gang and Wang, Kaixuan and Chen, Xiaozhi and Shen, Chunhua},
  year = {2023},
  pages = {9043--9053},
  code={https://github.com/YvanYin/Metric3D},
  preview={metric3d.jpg},
  award={Champion in CVPR2023 Monocular Depth Estimation Challenge},
  google_scholar_id={dhFuZR0502QC},
  selected={true}
}

@article{yinRevisitingOpenSetPanoptic2024,
  title = {Revisiting {{Open-Set Panoptic Segmentation}}},
  author = {Yin, Yufei and Chen, Hao and Zhou, Wengang and Deng, Jiajun and Xu, Haiming and Li, Houqiang},
  year = {2024},
  month = mar,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {38},
  number = {7},
  pages = {6747--6754},
  issn = {2374-3468},
  doi = {10.1609/aaai.v38i7.28498},
  urldate = {2025-02-17},
  abstract = {In this paper, we focus on the open-set panoptic segmentation (OPS) task to circumvent the data explosion problem. Different from the close-set setting, OPS targets to detect both known and unknown categories, where the latter is not annotated during training. Different from existing work that only selects a few common categories as unknown ones, we move forward to the real-world scenario by considering the various tail categories ({\textasciitilde}1k). To this end, we first build a new dataset with long-tail distribution for the OPS task. Based on this dataset, we additionally add a new class type for unknown classes and re-define the training annotations to make the OPS definition more complete and reasonable. Moreover, we analyze the influence of several significant factors in the OPS task and explore the upper bound of performance on unknown classes with different settings. Furthermore, based on the analyses, we design an effective two-phase framework for the OPS task, including thing-agnostic map generation and unknown segment mining. We further adopt semi-supervised learning to improve the OPS performance. Experimental results on different datasets validate the effectiveness of our method.},
  copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {CV: Segmentation},
  file = {C:\Users\stanz\Documents\Papers\Zotero\Projects\Hao\2024\Revisiting Open-Set Panoptic Segmentation.pdf}
}

@inproceedings{zhangLoRAPruneStructuredPruning2024,
  title = {{{LoRAPrune}}: {{Structured Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning}}},
  shorttitle = {{{LoRAPrune}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2024},
  author = {Zhang, Mingyang and Chen, Hao and Shen, Chunhua and Yang, Zhen and Ou, Linlin and Yu, Xinyi and Zhuang, Bohan},
  editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
  year = {2024},
  month = aug,
  pages = {3013--3026},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.findings-acl.178},
  urldate = {2025-02-17},
  abstract = {Large Language Models (LLMs), such as LLaMA and T5, have shown exceptional performance across various tasks through fine-tuning. Although low-rank adaption (LoRA) has emerged to cheaply fine-tune these LLMs on downstream tasks, their deployment is still hindered by the vast model scale and computational costs. Post-training model pruning offers a way to compress LLMs. However, the current pruning methods designed for LLMs are not compatible with LoRA. This is due to their utilization of unstructured pruning on LLMs, impeding the merging of LoRA weights, or their dependence on the gradients of pre-trained weights to guide pruning, which can impose significant memory overhead.To this end, we propose LoRAPrune, a new framework that delivers an accurate structured pruned model in a highly memory-efficient manner. Specifically, we first design a LoRA-guided pruning criterion, which uses the weights and gradients of LoRA, rather than the gradients of pre-trained weights for importance estimation. We subsequently integrate this criterion into an iterative pruning process, effectively removing redundant channels and heads. Extensive experimental results demonstrate the superior performance of our LoRAPrune over existing approaches on the LLaMA series models.At a 50\% compression rate, LoRAPrune demonstrates superior performance over LLM-Pruner, achieving a reduction in perplexity by 4.81 on WikiText2 and 3.46 on PTB, while also decreasing memory usage by 52.6\%.Besides, LoRAPrune also matches semi-structural pruning across multiple LLMs, proving its wide applicability. The code is available at https://github.com/aim-uofa/LoRAPrune.},
  file = {C:\Users\stanz\Documents\Papers\Zotero\Projects\Hao\2024\LoRAPrune Structured Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning.pdf}
}

@inproceedings{zhuSegPromptBoostingOpenWorld2023,
  abbr={ICCV},
  title = {{{SegPrompt}}: {{Boosting Open-World Segmentation}} via {{Category-Level Prompt Learning}}},
  shorttitle = {{{SegPrompt}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Zhu, Muzhi and Li, Hengtao and Chen, Hao and Fan, Chengxiang and Mao, Weian and Jing, Chenchen and Liu, Yifan and Shen, Chunhua},
  year = {2023},
  pages = {999--1008},
  urldate = {2025-02-17},
  langid = {english},
  file = {C:\Users\stanz\Documents\Papers\Zotero\Projects\Hao\2023\SegPrompt Boosting Open-World Segmentation via Category-Level Prompt Learning.pdf}
}

@article{zhuUnleashingPotentialDiffusion2025,
  title = {Unleashing the {{Potential}} of the {{Diffusion Model}} in {{Few-shot Semantic Segmentation}}},
  author = {Zhu, Muzhi and Liu, Yang and Luo, Zekai and Jing, Chenchen and Chen, Hao and Xu, Guangkai and Wang, Xinlong and Shen, Chunhua},
  year = {2024},
  month = jan,
  journal = {Advances in Neural Information Processing Systems},
  volume = {37},
  pages = {42672--42695},
  urldate = {2025-02-17},
  langid = {english},
  file = {C:\Users\stanz\Documents\Papers\Zotero\Projects\Hao\2025\Unleashing the Potential of the Diffusion Model in Few-shot Semantic Segmentation.pdf}
}
