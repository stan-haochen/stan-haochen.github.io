---
---

@inproceedings{zhang2025pomato,
  abbr={ICCV},
  title={POMATO: Marrying Pointmap Matching with Temporal Motion for Dynamic 3D Reconstruction},
  author={Zhang, Songyan and Ge, Yongtao and Tian, Jinyuan and Xu, Guangkai and Chen, Hao and Lv, Chen and Shen, Chunhua},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  year={2025},
  code = {https://github.com/wyddmw/POMATO},
  preview={pomato.png},
  selected={true}
}

@inproceedings{gao2025surfacesplat,
  abbr={ICCV},
  title={SurfaceSplat: Connecting Surface Reconstruction and Gaussian Splatting},
  author={Gao, Zihui and Bian, Jia-Wang and Lin, Guosheng and Chen, Hao and Shen, Chunhua},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  year={2025},
  html = {https://openreview.net/forum?id=hDIP1pQlQK},
  preview={surfacesplat.png}
}

@inproceedings{liu2025unified,
  abbr={ICCV},
  title={Unified Open-World Segmentation with Multi-Modal Prompts},
  author = {Liu, Yang and Yin, Yufei and Jing, Chenchen and Zhu, Muzhi and Chen, Hao and Xi, Yuling and Feng, Bo and Wang, Hao and Li, Shiyu and Shen, Chunhua},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  year={2025},
  html = {hhttps://openreview.net/forum?id=tpbvNY1nHL},
  preview={cosine.png}
}

@inproceedings{ge2025gvm,
  abbr={SIGGRAPH},
  author = {Ge, Yongtao and Xie, Kangyang and Xu, Guangkai and Ke, Li and Liu, Mingyu and Huang, Longtao and Xue, Hui and Chen, Hao and Shen, Chunhua},
  title = {Generative Video Matting},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3721238.3730642},
  doi = {10.1145/3721238.3730642},
  booktitle = {Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers},
  year={2025},
  series = {SIGGRAPH Conference Papers},
  code = {https://github.com/aim-uofa/GVM},
  preview={gvm.png}
}

@inproceedings{zhuSegAgentExploringPixel2025,
  abbr={CVPR},
  title = {{{SegAgent}}: {{Exploring Pixel Understanding Capabilities}} in {{MLLMs}} by {{Imitating Human Annotator Trajectories}}},
  shorttitle = {{{SegAgent}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Zhu, Muzhi and Tian, Yuzhuo and Chen, Hao and Zhou, Chunluan and Guo, Qingpei and Liu, Yang and Yang, Ming and Shen, Chunhua},
  year = {2025},
  pages = {3686--3696},
  urldate = {2025-08-20},
  preview = {segagent.png},
  code = {https://github.com/aim-uofa/SegAgent},
  website = {https://aim-uofa.github.io/SegAgent/},
  selected={true}
}

@inproceedings{boRevisitingConvolutionArchitecture2024,
  abbr={ICLR},
  title = {Revisiting {{Convolution Architecture}} in the {{Realm}} of {{DNA Foundation Models}}},
  booktitle = {The {{Thirteenth International Conference}} on {{Learning Representations}}},
  author = {Bo, Yu and Mao, Weian and Shao, Yanjun and Bai, Weiqiang and Ye, Peng and Ma, Xinzhu and Zhao, Junbo and Chen, Hao and Shen, Chunhua},
  year = {2025},
  preview = {convnova.jpg},
  code={https://github.com/aim-uofa/ConvNova},
  html={https://openreview.net/forum?id=B07dLVWLyD}
}

@inproceedings{chenPerturboLLaVAReducingMultimodal2024,
  abbr={ICLR},
  title = {{{PerturboLLaVA}}: {{Reducing Multimodal Hallucinations}} with {{Perturbative Visual Training}}},
  shorttitle = {{{PerturboLLaVA}}},
  booktitle = {The {{Thirteenth International Conference}} on {{Learning Representations}}},
  author = {Chen, Cong and Liu, Mingyu and Jing, Chenchen and Zhou, Yizhou and Rao, Fengyun and Chen, Hao and Zhang, Bo and Shen, Chunhua},
  year = {2025},
  code={https://github.com/aim-uofa/PerturboLLaVA},
  preview = {perturbollava.png}
}

@inproceedings{jiaoBoltzmannAlignedInverseFolding2024,
  abbr={ICLR},
  title = {Boltzmann-{{Aligned Inverse Folding Model}} as a {{Predictor}} of {{Mutational Effects}} on {{Protein-Protein Interactions}}},
  booktitle = {The {{Thirteenth International Conference}} on {{Learning Representations}}},
  author = {Jiao, Xiaoran and Mao, Weian and Jin, Wengong and Yang, Peiyuan and Chen, Hao and Shen, Chunhua},
  year = {2025},
  abstract = {Predicting the change in binding free energy (\${\textbackslash}Delta {\textbackslash}Delta G\$) is crucial for understanding and modulating protein-protein interactions, which are critical in drug design. Due to the scarcity of experimental \${\textbackslash}Delta{\textbackslash}Delta G\$ data, existing methods focus on pre-training, while alignment receives less attention. In this work, we propose the Boltzmann Alignment technique to transfer knowledge from pre-trained inverse folding models to \${\textbackslash}Delta{\textbackslash}Delta G\$ prediction. We begin by analyzing the thermodynamic definition of \${\textbackslash}Delta{\textbackslash}Delta G\$ and introducing the Boltzmann distribution to connect energy with protein conformational distribution. However, the protein conformational distribution is intractable; therefore, we employ Bayes' theorem to circumvent direct estimation and instead utilize the log-likelihood provided by protein inverse folding models for \${\textbackslash}Delta{\textbackslash}Delta G\$ estimation. Compared to previous inverse folding-based methods, our method explicitly accounts for the unbound state of protein complex in the \${\textbackslash}Delta {\textbackslash}Delta G\$ thermodynamic cycle, introducing a physical inductive bias and achieving both supervised and unsupervised state-of-the-art (SoTA) performance. Experimental results on SKEMPI v2 indicate that our method achieves Spearman coefficients of 0.3201 (unsupervised) and 0.5134 (supervised) on SKEMPI v2, significantly surpassing the previously reported SoTA values of 0.2632 and 0.4324, respectively. Futhermore, we demonstrate the capability of our method on binding energy prediction, protein-protein docking and antibody optimization tasks.},
  code={https://github.com/aim-uofa/BA-DDG},
  preview = {baddg.png}
}

@inproceedings{zhaoMovieDreamerHierarchicalGeneration2024,
  abbr={ICLR},
  title = {{{MovieDreamer}}: {{Hierarchical Generation}} for {{Coherent Long Visual Sequences}}},
  shorttitle = {{{MovieDreamer}}},
  booktitle = {The {{Thirteenth International Conference}} on {{Learning Representations}}},
  author = {Zhao, Canyu and Liu, Mingyu and Wang, Wen and Chen, Weihua and Wang, Fan and Chen, Hao and Zhang, Bo and Shen, Chunhua},
  year = {2025},
  code={https://github.com/aim-uofa/MovieDreamer},
  website={https://aim-uofa.github.io/MovieDreamer/},
  preview = {moviedreamer.png}
}

@inproceedings{wangFramerInteractiveFrame2024,
  abbr={ICLR},
  title = {Framer: {{Interactive Frame Interpolation}}},
  shorttitle = {Framer},
  booktitle = {The {{Thirteenth International Conference}} on {{Learning Representations}}},
  author = {Wang, Wen and Wang, Qiuyu and Zheng, Kecheng and Ouyang, Hao and Chen, Zhekai and Gong, Biao and Chen, Hao and Shen, Yujun and Shen, Chunhua},
  year = {2025},
  code={https://github.com/aim-uofa/Framer},
  website={https://aim-uofa.github.io/Framer/},
  preview={framer.png},
  video={https://youtu.be/4MPGKgn7jRc},
  selected={true}
}

@article{maLSSInstImprovingGeometric2024,
  abbr={3DV},
  title = {{{LSSInst}}: {{Improving Geometric Modeling}} in {{LSS-Based BEV Perception}} with {{Instance Representation}}},
  shorttitle = {{{LSSInst}}},
  author = {Ma, Weijie and Jiang, Jingwei and Yang, Yang and Chen, Zehui and Chen, Hao},
  year = {2024}
}

@inproceedings{zhuGenerativeActiveLearning2024,
  abbr={ICML},
  title = {Generative {{Active Learning}} for {{Long-tailed Instance Segmentation}}},
  booktitle = {Forty-First {{International Conference}} on {{Machine Learning}}},
  author = {Zhu, Muzhi and Fan, Chengxiang and Chen, Hao and Liu, Yang and Mao, Weian and Xu, Xiaogang and Shen, Chunhua},
  year = {2024},
  urldate = {2025-02-17},
  abstract = {Recently, large-scale language-image generative models have gained widespread attention and many works have utilized generated data from these models to further enhance the performance of perception tasks. However, not all generated data can positively impact downstream models, and these methods do not thoroughly explore how to better select and utilize generated data. On the other hand, there is still a lack of research oriented towards active learning on generated data. In this paper, we explore how to perform active learning specifically for generated data in the long-tailed instance segmentation task. Subsequently, we propose BSGAL, a new algorithm that estimates the contribution of the current batch-generated data based on gradient cache. BSGAL is meticulously designed to cater for unlimited generated data and complex downstream segmentation tasks. BSGAL outperforms the baseline approach and effectually improves the performance of long-tailed segmentation.},
  code={https://github.com/aim-uofa/DiverGen/tree/main/BSGAL},
  preview={bsgal.png},
  selected={true}
}

@inproceedings{liuFloatingAnchorDiffusion2024,
  abbr={ICML},
  title = {Floating {{Anchor Diffusion Model}} for {{Multi-motif Scaffolding}}},
  booktitle = {Forty-First {{International Conference}} on {{Machine Learning}}},
  author = {Liu, Ke and Mao, Weian and Shen, Shuaike and Jiao, Xiaoran and Sun, Zheng and Chen, Hao and Shen, Chunhua},
  year = {2024},
  code = {https://github.com/aim-uofa/FADiff}
}

@article{huMetric3DV2Versatile2024,
  abbr={TPAMI},
  title = {{{Metric3D}} v2: {{A Versatile Monocular Geometric Foundation Model}} for {{Zero-Shot Metric Depth}} and {{Surface Normal Estimation}}},
  shorttitle = {{{Metric3D}} V2},
  author = {Hu, Mu and Yin, Wei and Zhang, Chi and Cai, Zhipeng and Long, Xiaoxiao and Chen, Hao and Wang, Kaixuan and Yu, Gang and Shen, Chunhua and Shen, Shaojie},
  year = {2024},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {46},
  number = {12},
  pages = {10579--10596},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2024.3444912},
  abstract = {We introduce Metric3D v2, a geometric foundation model designed for zero-shot metric depth and surface normal estimation from single images, critical for accurate 3D recovery. Depth and normal estimation, though complementary, present distinct challenges. State-of-the-art monocular depth methods achieve zero-shot generalization through affine-invariant depths, but fail to recover real-world metric scale. Conversely, current normal estimation techniques struggle with zero-shot performance due to insufficient labeled data. We propose targeted solutions for both metric depth and normal estimation. For metric depth, we present a canonical camera space transformation module that resolves metric ambiguity across various camera models and large-scale datasets, which can be easily integrated into existing monocular models. For surface normal estimation, we introduce a joint depth-normal optimization module that leverages diverse data from metric depth, allowing normal estimators to improve beyond traditional labels. Our model, trained on over 16 million images from thousands of camera models with varied annotations, excels in zero-shot generalization to new camera settings. As shown in Fig. 1, It ranks the 1st in multiple zero-shot and standard benchmarks for metric depth and surface normal prediction. Our method enables the accurate recovery of metric 3D structures on randomly collected internet images, paving the way for plausible single-image metrology. Our model also relieves the scale drift issues of monocular-SLAM (Fig. 3), leading to high-quality metric scale dense mapping. Such applications highlight the versatility of Metric3D v2 models as geometric foundation models.},
  website = {https://jugghm.github.io/Metric3Dv2/},
  preview = {metric3dv2.png},
  video = {https://jugghm.github.io/Metric3Dv2/resource_new/media/demo_full.mp4}
}

@inproceedings{heDiffCalibReformulatingMonocular2024,
  abbr={AAAI},
  title = {{{DiffCalib}}: {{Reformulating Monocular Camera Calibration}} as {{Diffusion-Based Dense Incident Map Generation}}},
  shorttitle = {{{DiffCalib}}},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {He, Xiankang and Xu, Guangkai and Zhang, Bo and Chen, Hao and Cui, Ying and Guo, Dongyan},
  year = {2024},
  code={https://github.com/zjutcvg/DiffCalib},
  preview = {diffcalib.jpg}
}

@inproceedings{liuSimpleImageSegmentation2025,
  abbr={NeurIPS},
  title = {A {{Simple Image Segmentation Framework}} via {{In-Context Examples}}},
  author = {Liu, Yang and Jing, Chenchen and Li, Hengtao and Zhu, Muzhi and Chen, Hao and Wang, Xinlong and Shen, Chunhua},
  year = {2024},
  journal = {Advances in Neural Information Processing Systems},
  volume = {37},
  pages = {25095--25119},
  code={https://github.com/aim-uofa/SINE},
  preview={sine.png}
}

@article{zhuUnleashingPotentialDiffusion2025,
  abbr = {NeurIPS},
  title = {Unleashing the {{Potential}} of the {{Diffusion Model}} in {{Few-shot Semantic Segmentation}}},
  author = {Zhu, Muzhi and Liu, Yang and Luo, Zekai and Jing, Chenchen and Chen, Hao and Xu, Guangkai and Wang, Xinlong and Shen, Chunhua},
  year = {2024},
  journal = {Advances in Neural Information Processing Systems},
  volume = {37},
  pages = {42672--42695},
  code={https://github.com/aim-uofa/DiffewS},
  preview={diffews.png}
}

@article{yinRevisitingOpenSetPanoptic2024,
  abbr = {AAAI},
  title = {Revisiting {{Open-Set Panoptic Segmentation}}},
  author = {Yin, Yufei and Chen, Hao and Zhou, Wengang and Deng, Jiajun and Xu, Haiming and Li, Houqiang},
  year = {2024},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {38},
  number = {7},
  pages = {6747--6754},
  issn = {2374-3468},
  doi = {10.1609/aaai.v38i7.28498}
}

@article{jingRetrievalAugmentedPrimitiveRepresentations2024,
  abbr={AAAI},
  title = {Retrieval-{{Augmented Primitive Representations}} for {{Compositional Zero-Shot Learning}}},
  author = {Jing, Chenchen and Li, Yukun and Chen, Hao and Shen, Chunhua},
  year = {2024},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {38},
  number = {3},
  pages = {2652--2660},
  issn = {2374-3468},
  doi = {10.1609/aaai.v38i3.28043},
  code={https://github.com/jingchenchen/RAPR}
}

@inproceedings{zhangLoRAPruneStructuredPruning2024,
  title = {{{LoRAPrune}}: {{Structured Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning}}},
  shorttitle = {{{LoRAPrune}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2024},
  author = {Zhang, Mingyang and Chen, Hao and Shen, Chunhua and Yang, Zhen and Ou, Linlin and Yu, Xinyi and Zhuang, Bohan},
  editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
  year = {2024},
  month = aug,
  pages = {3013--3026},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.findings-acl.178},
  urldate = {2025-02-17},
  abstract = {Large Language Models (LLMs), such as LLaMA and T5, have shown exceptional performance across various tasks through fine-tuning. Although low-rank adaption (LoRA) has emerged to cheaply fine-tune these LLMs on downstream tasks, their deployment is still hindered by the vast model scale and computational costs. Post-training model pruning offers a way to compress LLMs. However, the current pruning methods designed for LLMs are not compatible with LoRA. This is due to their utilization of unstructured pruning on LLMs, impeding the merging of LoRA weights, or their dependence on the gradients of pre-trained weights to guide pruning, which can impose significant memory overhead.To this end, we propose LoRAPrune, a new framework that delivers an accurate structured pruned model in a highly memory-efficient manner. Specifically, we first design a LoRA-guided pruning criterion, which uses the weights and gradients of LoRA, rather than the gradients of pre-trained weights for importance estimation. We subsequently integrate this criterion into an iterative pruning process, effectively removing redundant channels and heads. Extensive experimental results demonstrate the superior performance of our LoRAPrune over existing approaches on the LLaMA series models.At a 50\% compression rate, LoRAPrune demonstrates superior performance over LLM-Pruner, achieving a reduction in perplexity by 4.81 on WikiText2 and 3.46 on PTB, while also decreasing memory usage by 52.6\%.Besides, LoRAPrune also matches semi-structural pruning across multiple LLMs, proving its wide applicability. The code is available at https://github.com/aim-uofa/LoRAPrune.},
  file = {C:\Users\stanz\Documents\Papers\Zotero\Projects\Hao\2024\LoRAPrune Structured Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning.pdf}
}

@inproceedings{chenFreeComposeGenericZeroShot2025,
  abbr={ECCV},
  title = {{{FreeCompose}}: {{Generic Zero-Shot Image Composition}} with~{{Diffusion Prior}}},
  shorttitle = {{{FreeCompose}}},
  booktitle = {The 17th {{European Conference}} on {{Computer Vision ECCV}} 2024},
  author = {Chen, Zhekai and Wang, Wen and Yang, Zhen and Yuan, Zeqing and Chen, Hao and Shen, Chunhua},
  year = {2024},
  code={https://github.com/aim-uofa/FreeCompose},
  preview = {freecompose.png}
}

@inproceedings{dingFreeCustomTuningFreeCustomized2024,
  abbr={CVPR},
  title = {{{FreeCustom}}: {{Tuning-Free Customized Image Generation}} for {{Multi-Concept Composition}}},
  shorttitle = {{{FreeCustom}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Ding, Ganggui and Zhao, Canyu and Wang, Wen and Yang, Zhen and Liu, Zide and Chen, Hao and Shen, Chunhua},
  year = {2024},
  code={https://github.com/aim-uofa/FreeCustom},
  website={https://aim-uofa.github.io/FreeCustom/},
  preview = {freecustom.png}
}

@inproceedings{fanDiverGenImprovingInstance2024,
  abbr={CVPR},
  title = {{{DiverGen}}: {{Improving Instance Segmentation}} by {{Learning Wider Data Distribution}} with {{More Diverse Generative Data}}},
  shorttitle = {{{DiverGen}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Fan, Chengxiang and Zhu, Muzhi and Chen, Hao and Liu, Yang and Wu, Weijia and Zhang, Huaqi and Shen, Chunhua},
  year = {2024},
  code={https://github.com/aim-uofa/DiverGen/tree/main/DiverGen},
  preview = {divergen.png}
}

@inproceedings{liuMatcherSegmentAnything2023,
  abbr={ICLR},
  title = {Matcher: {{Segment Anything}} with {{One Shot Using All-Purpose Feature Matching}}},
  shorttitle = {Matcher},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Liu, Yang and Zhu, Muzhi and Li, Hengtao and Chen, Hao and Wang, Xinlong and Shen, Chunhua},
  year = {2024},
  code = {https://github.com/aim-uofa/Matcher},
  preview = {matcher.png},
  selected={true}
}

@article{wangAutoStoryGeneratingDiverse2024,
  abbr={IJCV},
  title = {{{AutoStory}}: {{Generating Diverse Storytelling Images}} with {{Minimal Human Efforts}}},
  shorttitle = {{{AutoStory}}},
  author = {Wang, Wen and Zhao, Canyu and Chen, Hao and Chen, Zhekai and Zheng, Kecheng and Shen, Chunhua},
  year = {2024},
  journal = {International Journal of Computer Vision},
  issn = {1573-1405},
  doi = {10.1007/s11263-024-02309-y},
  website = {https://aim-uofa.github.io/AutoStory/},
  code = {https://github.com/aim-uofa/AutoStory},
  preview={autostory.png}
}

@inproceedings{maoNovoProteinDesign2023,
  abbr={ICLR},
  title = {De Novo {{Protein Design Using Geometric Vector Field Networks}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Mao, Weian and Zhu, Muzhi and Sun, Zheng and Shen, Shuaike and Wu, Lin Yuanbo and Chen, Hao and Shen, Chunhua},
  year = {2024},
  urldate = {2025-02-17},
  abstract = {Advances like protein diffusion have marked revolutionary progress in \${\textbackslash}textit\{de novo\}\$ protein design, a central topic in life science. These methods typically depend on protein structure encoders to model residue backbone frames, where atoms do not exist. Most prior encoders rely on atom-wise features, such as angles and distances between atoms, which are not available in this context. Only a few basic encoders, like IPA, have been proposed for this scenario, exposing the frame modeling as a bottleneck. In this work, we introduce the Vector Field Network (VFN), that enables network layers to perform learnable vector computations between coordinates of frame-anchored virtual atoms, thus achieving a higher capability for modeling frames. The vector computation operates in a manner similar to a linear layer, with each input channel receiving 3D virtual atom coordinates instead of scalar values. The multiple feature vectors output by the vector computation are then used to update the residue representations and virtual atom coordinates via attention aggregation. Remarkably, VFN also excels in modeling both frames and atoms, as the real atoms can be treated as the virtual atoms for modeling, positioning VFN as a potential \${\textbackslash}textit\{universal encoder\}\$. In protein diffusion (frame modeling), VFN exhibits a impressive performance advantage over IPA, excelling in terms of both designability (\${\textbackslash}textbf\{67.04\}\${\textbackslash}\% vs. 53.58{\textbackslash}\%) and diversity (\${\textbackslash}textbf\{66.54\}\${\textbackslash}\% vs. 51.98{\textbackslash}\%). In inverse folding(frame and atom modeling), VFN outperforms the previous SoTA model, PiFold (\${\textbackslash}textbf\{54.7\}\${\textbackslash}\% vs. 51.66{\textbackslash}\%), on sequence recovery rate; we also propose a method of equipping VFN with the ESM model, which significantly surpasses the previous ESM-based SoTA (\${\textbackslash}textbf\{62.67\}\${\textbackslash}\% vs. 55.65{\textbackslash}\%), LM-Design, by a substantial margin. Code is available at https://github.com/aim-uofa/VFN},
  langid = {english},
  file = {C:\Users\stanz\Documents\Papers\Zotero\Projects\Hao\2023\De novo Protein Design Using Geometric Vector Field Networks.pdf}
}

@article{wuDatasetDMSynthesizingData2023,
  abbr={NeurIPS},
  title = {{{DatasetDM}}: {{Synthesizing Data}} with {{Perception Annotations Using Diffusion Models}}},
  shorttitle = {{{DatasetDM}}},
  author = {Wu, Weijia and Zhao, Yuzhong and Chen, Hao and Gu, Yuchao and Zhao, Rui and He, Yefei and Zhou, Hong and Shou, Mike Zheng and Shen, Chunhua},
  year = {2023},
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {54683--54695},
  code = {https://github.com/showlab/DatasetDM},
  website = {https://weijiawu.github.io/DatasetDM_page/},
  preview = {datasetdm.jpg}
}

@inproceedings{liLearningFuseMonocular2023,
  abbr={CVPR},
  title = {Learning {{To Fuse Monocular}} and {{Multi-View Cues}} for {{Multi-Frame Depth Estimation}} in {{Dynamic Scenes}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Li, Rui and Gong, Dong and Yin, Wei and Chen, Hao and Zhu, Yu and Wang, Kaixuan and Chen, Xiaozhi and Sun, Jinqiu and Zhang, Yanning},
  year = {2023},
  pages = {21539--21548}
}


@inproceedings{wangLearningConditionalAttributes2023,
  abbr={CVPR},
  title = {Learning {{Conditional Attributes}} for {{Compositional Zero-Shot Learning}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Wang, Qingsheng and Liu, Lingqiao and Jing, Chenchen and Chen, Hao and Liang, Guoqiang and Wang, Peng and Shen, Chunhua},
  year = {2023},
  pages = {11197--11206}
}

@article{xiDynamicFeatureInteraction2023,
  abbr={IJCV},
  title = {A {{Dynamic Feature Interaction Framework}} for {{Multi-task Visual Perception}}},
  author = {Xi, Yuling and Chen, Hao and Wang, Ning and Wang, Peng and Zhang, Yanning and Shen, Chunhua and Liu, Yifan},
  year = {2023},
  journal = {International Journal of Computer Vision},
  volume = {131},
  number = {11},
  pages = {2977--2993},
  issn = {1573-1405},
  doi = {10.1007/s11263-023-01835-5}
}

@inproceedings{xuFrozenReconPosefree3D2023,
  abbr={ICCV},
  title = {{{FrozenRecon}}: {{Pose-free 3D Scene Reconstruction}} with {{Frozen Depth Models}}},
  shorttitle = {{{FrozenRecon}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Xu, Guangkai and Yin, Wei and Chen, Hao and Shen, Chunhua and Cheng, Kai and Zhao, Feng},
  year = {2023},
  pages = {9310--9320},
  code = {https://github.com/aim-uofa/FrozenRecon},
  website = {https://aim-uofa.github.io/FrozenRecon/},
  preview = {frozenrecon.png},
  selected={true}
}

@inproceedings{xuWhatMattersWhen2024,
  abbr={ICLR},
  title = {What {{Matters When Repurposing Diffusion Models}} for {{General Dense Perception Tasks}}?},
  booktitle = {The {{Thirteenth International Conference}} on {{Learning Representations}}},
  author = {Xu, Guangkai and Ge, Yongtao and Liu, Mingyu and Fan, Chengxiang and Xie, Kangyang and Zhao, Zhiyue and Chen, Hao and Shen, Chunhua},
  year = {2024},
  code = {https://github.com/aim-uofa/GenPercept},
  preview = {genpercept.jpg},
  selected={true}
}

@inproceedings{yangObjectAwareInversionReassembly2023,
  abbr={ICLR},
  title = {Object-{{Aware Inversion}} and {{Reassembly}} for {{Image Editing}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Yang, Zhen and Ding, Ganggui and Wang, Wen and Chen, Hao and Zhuang, Bohan and Shen, Chunhua},
  year = {2023},
  code = {https://github.com/aim-uofa/OIR},
  website = {https://aim-uofa.github.io/OIR-Diffusion/},
  video = {https://iclr.cc/virtual/2024/poster/18242},
  preview = {oir.png}
}

@inproceedings{yingCTVISConsistentTraining2023,
  abbr={ICCV},
  title = {{{CTVIS}}: {{Consistent Training}} for {{Online Video Instance Segmentation}}},
  shorttitle = {{{CTVIS}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Ying, Kaining and Zhong, Qing and Mao, Weian and Wang, Zhenhua and Chen, Hao and Wu, Lin Yuanbo and Liu, Yifan and Fan, Chengxiang and Zhuge, Yunzhi and Shen, Chunhua},
  year = {2023},
  pages = {899--908}
}

@inproceedings{yinMetric3DZeroshotMetric2023,
  abbr={ICCV},
  title = {{{Metric3D}}: {{Towards Zero-shot Metric 3D Prediction}} from {{A Single Image}}},
  shorttitle = {{{Metric3D}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Yin, Wei and Zhang, Chi and Chen, Hao and Cai, Zhipeng and Yu, Gang and Wang, Kaixuan and Chen, Xiaozhi and Shen, Chunhua},
  year = {2023},
  pages = {9043--9053},
  code={https://github.com/YvanYin/Metric3D},
  preview={metric3d.jpg},
  award={Champion in CVPR2023 Monocular Depth Estimation Challenge},
  google_scholar_id={dhFuZR0502QC},
  selected={true}
}

@inproceedings{zhuSegPromptBoostingOpenWorld2023,
  abbr={ICCV},
  title = {{{SegPrompt}}: {{Boosting Open-World Segmentation}} via {{Category-Level Prompt Learning}}},
  shorttitle = {{{SegPrompt}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Zhu, Muzhi and Li, Hengtao and Chen, Hao and Fan, Chengxiang and Mao, Weian and Jing, Chenchen and Liu, Yifan and Shen, Chunhua},
  year = {2023},
  pages = {999--1008}
}
